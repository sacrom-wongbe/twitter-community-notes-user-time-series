{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc01b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Combining all noteRatings parquet files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381c27a196f145d6bff5b26c68807dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Master parquet ready â†’ C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\\ratings_raw\\noteRatings_master.parquet\n"
     ]
    }
   ],
   "source": [
    "import os, duckdb\n",
    "\n",
    "base = r\"C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\\ratings_raw\"\n",
    "out_path = os.path.join(base, \"noteRatings_master.parquet\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"PRAGMA memory_limit='2GB';\")\n",
    "con.execute(f\"PRAGMA temp_directory='{base}';\")\n",
    "\n",
    "print(\"ðŸš€ Combining all noteRatings parquet files...\")\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  SELECT * FROM read_parquet('{base}\\\\noteRatings-*.parquet')\n",
    ") TO '{out_path}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "con.close()\n",
    "print(f\"âœ… Master parquet ready â†’ {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43add4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Loading and analyzing: C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\\ratings_raw\\noteRatings_master.parquet\n",
      "\n",
      "Total rows: 171,877,798\n",
      "\n",
      "Column names and types:\n",
      "                         column_name column_type null  key default extra\n",
      "                              noteId      BIGINT  YES None    None  None\n",
      "                  raterParticipantId     VARCHAR  YES None    None  None\n",
      "                     createdAtMillis      BIGINT  YES None    None  None\n",
      "                             version      BIGINT  YES None    None  None\n",
      "                               agree      BIGINT  YES None    None  None\n",
      "                            disagree      BIGINT  YES None    None  None\n",
      "                             helpful      BIGINT  YES None    None  None\n",
      "                          notHelpful      BIGINT  YES None    None  None\n",
      "                    helpfulnessLevel     VARCHAR  YES None    None  None\n",
      "                        helpfulOther      BIGINT  YES None    None  None\n",
      "                  helpfulInformative      BIGINT  YES None    None  None\n",
      "                        helpfulClear      BIGINT  YES None    None  None\n",
      "                   helpfulEmpathetic      BIGINT  YES None    None  None\n",
      "                  helpfulGoodSources      BIGINT  YES None    None  None\n",
      "                helpfulUniqueContext      BIGINT  YES None    None  None\n",
      "               helpfulAddressesClaim      BIGINT  YES None    None  None\n",
      "             helpfulImportantContext      BIGINT  YES None    None  None\n",
      "             helpfulUnbiasedLanguage      BIGINT  YES None    None  None\n",
      "                     notHelpfulOther      BIGINT  YES None    None  None\n",
      "                 notHelpfulIncorrect      BIGINT  YES None    None  None\n",
      "notHelpfulSourcesMissingOrUnreliable      BIGINT  YES None    None  None\n",
      "  notHelpfulOpinionSpeculationOrBias      BIGINT  YES None    None  None\n",
      "          notHelpfulMissingKeyPoints      BIGINT  YES None    None  None\n",
      "                  notHelpfulOutdated      BIGINT  YES None    None  None\n",
      "          notHelpfulHardToUnderstand      BIGINT  YES None    None  None\n",
      "     notHelpfulArgumentativeOrBiased      BIGINT  YES None    None  None\n",
      "                  notHelpfulOffTopic      BIGINT  YES None    None  None\n",
      "     notHelpfulSpamHarassmentOrAbuse      BIGINT  YES None    None  None\n",
      "         notHelpfulIrrelevantSources      BIGINT  YES None    None  None\n",
      "        notHelpfulOpinionSpeculation      BIGINT  YES None    None  None\n",
      "             notHelpfulNoteNotNeeded      BIGINT  YES None    None  None\n",
      "                      ratedOnTweetId      BIGINT  YES None    None  None\n",
      "             _processing_commit_hash     VARCHAR  YES None    None  None\n",
      "                       _processed_at     VARCHAR  YES None    None  None\n",
      "                          _data_date     VARCHAR  YES None    None  None\n",
      "\n",
      "Total columns: 35\n",
      "\n",
      "Preview of first 5 rows:\n",
      "                noteId                                 raterParticipantId  \\\n",
      "0  1714589572200304695  C784F04F26E124F4D6EC01658D8F5565005D3092741FB3...   \n",
      "1  1727529309810655291  C784F04F26E124F4D6EC01658D8F5565005D3092741FB3...   \n",
      "2  1728563772543910115  C784F04F26E124F4D6EC01658D8F5565005D3092741FB3...   \n",
      "3  1730887409146175557  C784F04F26E124F4D6EC01658D8F5565005D3092741FB3...   \n",
      "4  1733821630449668329  C784F04F26E124F4D6EC01658D8F5565005D3092741FB3...   \n",
      "\n",
      "   createdAtMillis  version  agree  disagree  helpful  notHelpful  \\\n",
      "0    1697637486068        2      0         0        0           0   \n",
      "1    1700865444499        2      0         0        0           0   \n",
      "2    1701005059910        2      0         0        0           0   \n",
      "3    1701610674078        2      0         0        0           0   \n",
      "4    1702228844546        2      0         0        0           0   \n",
      "\n",
      "   helpfulnessLevel  helpfulOther  ...  notHelpfulArgumentativeOrBiased  \\\n",
      "0           HELPFUL             0  ...                                0   \n",
      "1           HELPFUL             0  ...                                0   \n",
      "2  SOMEWHAT_HELPFUL             0  ...                                0   \n",
      "3           HELPFUL             0  ...                                0   \n",
      "4           HELPFUL             0  ...                                0   \n",
      "\n",
      "   notHelpfulOffTopic  notHelpfulSpamHarassmentOrAbuse  \\\n",
      "0                   0                                0   \n",
      "1                   0                                0   \n",
      "2                   0                                0   \n",
      "3                   0                                0   \n",
      "4                   0                                0   \n",
      "\n",
      "   notHelpfulIrrelevantSources  notHelpfulOpinionSpeculation  \\\n",
      "0                            0                             0   \n",
      "1                            0                             0   \n",
      "2                            0                             0   \n",
      "3                            0                             0   \n",
      "4                            0                             0   \n",
      "\n",
      "   notHelpfulNoteNotNeeded       ratedOnTweetId  \\\n",
      "0                        0  1714342122185191596   \n",
      "1                        0  1727406627211330041   \n",
      "2                        0  1728535065242612184   \n",
      "3                        0  1730650140229201949   \n",
      "4                        0  1733581286256607619   \n",
      "\n",
      "                    _processing_commit_hash               _processed_at  \\\n",
      "0  8be5ec1f5981a578cc3ab7224eecbf9f7ae5564c  2025-09-27T00:04:24.795537   \n",
      "1  8be5ec1f5981a578cc3ab7224eecbf9f7ae5564c  2025-09-27T00:04:24.795537   \n",
      "2  8be5ec1f5981a578cc3ab7224eecbf9f7ae5564c  2025-09-27T00:04:24.795537   \n",
      "3  8be5ec1f5981a578cc3ab7224eecbf9f7ae5564c  2025-09-27T00:04:24.795537   \n",
      "4  8be5ec1f5981a578cc3ab7224eecbf9f7ae5564c  2025-09-27T00:04:24.795537   \n",
      "\n",
      "   _data_date  \n",
      "0  2025-09-27  \n",
      "1  2025-09-27  \n",
      "2  2025-09-27  \n",
      "3  2025-09-27  \n",
      "4  2025-09-27  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Path to the master parquet file\n",
    "base = r\"C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\\ratings_raw\"\n",
    "master_path = os.path.join(base, \"noteRatings_master.parquet\")\n",
    "\n",
    "print(f\"ðŸ“Š Loading and analyzing: {master_path}\\n\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Get row count\n",
    "row_count = con.execute(f\"SELECT COUNT(*) FROM '{master_path}'\").fetchone()[0]\n",
    "print(f\"Total rows: {row_count:,}\\n\")\n",
    "\n",
    "# Get column names and types\n",
    "print(\"Column names and types:\")\n",
    "schema = con.execute(f\"DESCRIBE SELECT * FROM '{master_path}'\").fetchdf()\n",
    "print(schema.to_string(index=False))\n",
    "print(f\"\\nTotal columns: {len(schema)}\\n\")\n",
    "\n",
    "# Get basic statistics\n",
    "print(\"Preview of first 5 rows:\")\n",
    "preview = con.execute(f\"SELECT * FROM '{master_path}' LIMIT 5\").fetchdf()\n",
    "print(preview)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f131034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Loading and analyzing: C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\\scored_notes_2dim.parquet\n",
      "\n",
      "Total rows: 2,339,700\n",
      "\n",
      "Column names and types:\n",
      "                      column_name column_type null  key default extra\n",
      "                           noteId      BIGINT  YES None    None  None\n",
      "                coreNoteIntercept       FLOAT  YES None    None  None\n",
      "                  coreNoteFactor1       FLOAT  YES None    None  None\n",
      "                finalRatingStatus     VARCHAR  YES None    None  None\n",
      "                         firstTag     VARCHAR  YES None    None  None\n",
      "                        secondTag     VARCHAR  YES None    None  None\n",
      "                  coreActiveRules     VARCHAR  YES None    None  None\n",
      "                 activeFilterTags     VARCHAR  YES None    None  None\n",
      "                   classification     VARCHAR  YES None    None  None\n",
      "                  createdAtMillis      DOUBLE  YES None    None  None\n",
      "                 coreRatingStatus     VARCHAR  YES None    None  None\n",
      "            metaScorerActiveRules     VARCHAR  YES None    None  None\n",
      "                        decidedBy     VARCHAR  YES None    None  None\n",
      "           expansionNoteIntercept       FLOAT  YES None    None  None\n",
      "             expansionNoteFactor1       FLOAT  YES None    None  None\n",
      "            expansionRatingStatus     VARCHAR  YES None    None  None\n",
      "            coverageNoteIntercept       FLOAT  YES None    None  None\n",
      "              coverageNoteFactor1       FLOAT  YES None    None  None\n",
      "             coverageRatingStatus     VARCHAR  YES None    None  None\n",
      "             coreNoteInterceptMin       FLOAT  YES None    None  None\n",
      "             coreNoteInterceptMax       FLOAT  YES None    None  None\n",
      "        expansionNoteInterceptMin      DOUBLE  YES None    None  None\n",
      "        expansionNoteInterceptMax      DOUBLE  YES None    None  None\n",
      "         coverageNoteInterceptMin      DOUBLE  YES None    None  None\n",
      "         coverageNoteInterceptMax      DOUBLE  YES None    None  None\n",
      "               groupNoteIntercept      DOUBLE  YES None    None  None\n",
      "                 groupNoteFactor1      DOUBLE  YES None    None  None\n",
      "                groupRatingStatus     VARCHAR  YES None    None  None\n",
      "            groupNoteInterceptMax      DOUBLE  YES None    None  None\n",
      "            groupNoteInterceptMin      DOUBLE  YES None    None  None\n",
      "                    modelingGroup      DOUBLE  YES None    None  None\n",
      "                       numRatings      BIGINT  YES None    None  None\n",
      "   timestampMillisOfCurrentStatus      DOUBLE  YES None    None  None\n",
      "       expansionPlusNoteIntercept       FLOAT  YES None    None  None\n",
      "         expansionPlusNoteFactor1       FLOAT  YES None    None  None\n",
      "        expansionPlusRatingStatus     VARCHAR  YES None    None  None\n",
      "               topicNoteIntercept      DOUBLE  YES None    None  None\n",
      "                 topicNoteFactor1      DOUBLE  YES None    None  None\n",
      "                topicRatingStatus     VARCHAR  YES None    None  None\n",
      "                        noteTopic     VARCHAR  YES None    None  None\n",
      "               topicNoteConfident     BOOLEAN  YES None    None  None\n",
      "             expansionActiveRules     VARCHAR  YES None    None  None\n",
      "         expansionPlusActiveRules     VARCHAR  YES None    None  None\n",
      "                 groupActiveRules     VARCHAR  YES None    None  None\n",
      "                 topicActiveRules     VARCHAR  YES None    None  None\n",
      "         coreNumFinalRoundRatings      DOUBLE  YES None    None  None\n",
      "    expansionNumFinalRoundRatings      DOUBLE  YES None    None  None\n",
      "expansionPlusNumFinalRoundRatings      DOUBLE  YES None    None  None\n",
      "        groupNumFinalRoundRatings      DOUBLE  YES None    None  None\n",
      "        topicNumFinalRoundRatings      DOUBLE  YES None    None  None\n",
      "             rescoringActiveRules     VARCHAR  YES None    None  None\n",
      "          multiGroupNoteIntercept      DOUBLE  YES None    None  None\n",
      "            multiGroupNoteFactor1      DOUBLE  YES None    None  None\n",
      "           multiGroupRatingStatus     VARCHAR  YES None    None  None\n",
      "               modelingMultiGroup      DOUBLE  YES None    None  None\n",
      "            multiGroupActiveRules     VARCHAR  YES None    None  None\n",
      "   multiGroupNumFinalRoundRatings      DOUBLE  YES None    None  None\n",
      "\n",
      "Total columns: 57\n",
      "\n",
      "Preview of first 5 rows:\n",
      "                noteId  coreNoteIntercept  coreNoteFactor1  \\\n",
      "0  1352796878438424576           0.118955        -0.214416   \n",
      "1  1353415873227177985           0.090493        -0.133521   \n",
      "2  1354586938863443971                NaN              NaN   \n",
      "3  1354588003075764229                NaN              NaN   \n",
      "4  1354588172659920899                NaN              NaN   \n",
      "\n",
      "    finalRatingStatus firstTag secondTag  \\\n",
      "0  NEEDS_MORE_RATINGS     None      None   \n",
      "1  NEEDS_MORE_RATINGS     None      None   \n",
      "2  NEEDS_MORE_RATINGS     None      None   \n",
      "3  NEEDS_MORE_RATINGS     None      None   \n",
      "4  NEEDS_MORE_RATINGS     None      None   \n",
      "\n",
      "                               coreActiveRules activeFilterTags  \\\n",
      "0  InitialNMR (v1.0),RejectLowIntercept (v1.0)             None   \n",
      "1  InitialNMR (v1.0),RejectLowIntercept (v1.0)             None   \n",
      "2                            InitialNMR (v1.0)             None   \n",
      "3                                         None             None   \n",
      "4                                         None             None   \n",
      "\n",
      "                          classification  createdAtMillis  ...  \\\n",
      "0                                   None     1.611367e+12  ...   \n",
      "1                                   None     1.611514e+12  ...   \n",
      "2  MISINFORMED_OR_POTENTIALLY_MISLEADING     1.611794e+12  ...   \n",
      "3                         NOT_MISLEADING     1.611794e+12  ...   \n",
      "4                         NOT_MISLEADING     1.611794e+12  ...   \n",
      "\n",
      "  expansionPlusNumFinalRoundRatings groupNumFinalRoundRatings  \\\n",
      "0                               5.0                       5.0   \n",
      "1                               5.0                       5.0   \n",
      "2                               1.0                       1.0   \n",
      "3                               NaN                       NaN   \n",
      "4                               NaN                       NaN   \n",
      "\n",
      "  topicNumFinalRoundRatings  rescoringActiveRules  multiGroupNoteIntercept  \\\n",
      "0                       NaN             ALL_NOTES                      NaN   \n",
      "1                       NaN             ALL_NOTES                      NaN   \n",
      "2                       NaN             ALL_NOTES                      NaN   \n",
      "3                       NaN             ALL_NOTES                      NaN   \n",
      "4                       NaN             ALL_NOTES                      NaN   \n",
      "\n",
      "  multiGroupNoteFactor1  multiGroupRatingStatus  modelingMultiGroup  \\\n",
      "0                   NaN                    None                 NaN   \n",
      "1                   NaN                    None                 NaN   \n",
      "2                   NaN                    None                 NaN   \n",
      "3                   NaN                    None                 NaN   \n",
      "4                   NaN                    None                 NaN   \n",
      "\n",
      "  multiGroupActiveRules  multiGroupNumFinalRoundRatings  \n",
      "0                  None                             NaN  \n",
      "1                  None                             NaN  \n",
      "2                  None                             NaN  \n",
      "3                  None                             NaN  \n",
      "4                  None                             NaN  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "# Path to the master parquet file\n",
    "base = r\"C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\"\n",
    "master_path = os.path.join(base, \"scored_notes_2dim.parquet\")\n",
    "\n",
    "print(f\"ðŸ“Š Loading and analyzing: {master_path}\\n\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Get row count\n",
    "row_count = con.execute(f\"SELECT COUNT(*) FROM '{master_path}'\").fetchone()[0]\n",
    "print(f\"Total rows: {row_count:,}\\n\")\n",
    "\n",
    "# Get column names and types\n",
    "print(\"Column names and types:\")\n",
    "schema = con.execute(f\"DESCRIBE SELECT * FROM '{master_path}'\").fetchdf()\n",
    "print(schema.to_string(index=False))\n",
    "print(f\"\\nTotal columns: {len(schema)}\\n\")\n",
    "\n",
    "# Get basic statistics\n",
    "print(\"Preview of first 5 rows:\")\n",
    "preview = con.execute(f\"SELECT * FROM '{master_path}' LIMIT 5\").fetchdf()\n",
    "print(preview)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "847f798d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rating rows: 171,877,798\n",
      "\n",
      "ðŸš€ Joining batch 1 (offset 0)\n",
      "\n",
      "ðŸš€ Joining batch 2 (offset 5,000,000)\n",
      "\n",
      "ðŸš€ Joining batch 2 (offset 5,000,000)\n",
      "\n",
      "ðŸš€ Joining batch 3 (offset 10,000,000)\n",
      "\n",
      "ðŸš€ Joining batch 3 (offset 10,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7d23a879884d6eb0d143ac26a26ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 4 (offset 15,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092fc00c63ca423988f64626bfab70ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 5 (offset 20,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afeec222e51d44e8868210e6636a172e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 6 (offset 25,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30578b59ea5741fd9250f6f3783c4924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 7 (offset 30,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4402fc73d61b407683b6bf1b4ec88df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 8 (offset 35,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3802d2780a4062a55f3b828c746e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 9 (offset 40,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826f4c5eed3748a5aa75f94f09ac8dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 10 (offset 45,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc1f1834e464bd6a14d816ba33d977f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 11 (offset 50,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915feb0dce92489b9787dbd578031969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 12 (offset 55,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d45aec7344d4d64a3a497011ba4efe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 13 (offset 60,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecae00d68a6f484d9f6ef503f68eb7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 14 (offset 65,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf85aa378fe4284ace48346088338f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 15 (offset 70,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bd962afef4423ea6cc61577bdd5dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 16 (offset 75,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8d427dbc964bc0b3177b66d09401b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 17 (offset 80,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c1c73de81245409fcab75778e08aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 18 (offset 85,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40fb91444904537956f31b9c3b4f054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 19 (offset 90,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a783d12b2b84a84b9e93bcec7961a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 20 (offset 95,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d1e0728d9e42a4a85fc7365cb247f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 21 (offset 100,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab24b3673df4656804d13f7e65e62fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 22 (offset 105,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159f5dbb95654bc8b0fda2022a784146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 23 (offset 110,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed5b1ac74244f508d864fb6314cd74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 24 (offset 115,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e73d91d845f4e3f8af8919e998ce494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 25 (offset 120,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8febbf14e5184272b9e8e710fbee3255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 26 (offset 125,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d2e8ffbcad43049aa4586092909f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 27 (offset 130,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223c31397694465abe10b84088c94f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 28 (offset 135,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4156571861c944c796d1265865265bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 29 (offset 140,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cce3f890f54e93a3d986dfe6934bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 30 (offset 145,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65c89c944ab4d8ab4a68dde3223bbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 31 (offset 150,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30e36c12f9f4642968e8e8c2d69e6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 32 (offset 155,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15838b89d94a4464982b3b5783790feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 33 (offset 160,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e20fc61b584cf6bf54dd8154ed938e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 34 (offset 165,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315eb45f106247d4aa5afd1db04c6df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Joining batch 35 (offset 170,000,000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8df94b147d43bab392f2c77dd039af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§© Combining all parts into one parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03819d24d3834fe6a11b91cb75e99959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done. ratings_with_scores.parquet written to C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\\intermediates\\ratings_with_scores.parquet\n"
     ]
    }
   ],
   "source": [
    "import os, gc, duckdb\n",
    "\n",
    "# --- Paths ---\n",
    "base = r\"C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\"\n",
    "ratings_dir = os.path.join(base, \"ratings_raw\")\n",
    "scores_path  = os.path.join(base, \"scored_notes_2dim.parquet\")\n",
    "\n",
    "output_dir = os.path.join(base, \"intermediates\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "TEMP_DIR = os.path.join(output_dir, \"tmp_join\")\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "out_path = os.path.join(output_dir, \"ratings_with_scores.parquet\")\n",
    "\n",
    "# --- Settings ---\n",
    "MEM_LIMIT_GB = 4\n",
    "BATCH_SIZE   = 5_000_000  # tune to fit your RAM\n",
    "\n",
    "# --- Connect and count total rows across all parquet partitions ---\n",
    "con = duckdb.connect()\n",
    "con.execute(f\"PRAGMA memory_limit='{MEM_LIMIT_GB}GB';\")\n",
    "con.execute(f\"PRAGMA temp_directory='{TEMP_DIR}';\")\n",
    "\n",
    "total_rows = con.execute(\n",
    "    f\"SELECT SUM(row_count) FROM (SELECT COUNT(*) AS row_count FROM read_parquet('{ratings_dir}\\\\noteRatings-*.parquet'))\"\n",
    ").fetchone()[0]\n",
    "\n",
    "print(f\"Total rating rows: {total_rows:,}\")\n",
    "con.close()\n",
    "\n",
    "# --- Process in batches ---\n",
    "offset = 0\n",
    "batch_idx = 0\n",
    "part_files = []\n",
    "\n",
    "while offset < total_rows:\n",
    "    batch_idx += 1\n",
    "    out_part = os.path.join(TEMP_DIR, f\"ratings_scores_part_{batch_idx:03d}.parquet\")\n",
    "    print(f\"\\nðŸš€ Joining batch {batch_idx} (offset {offset:,})\")\n",
    "\n",
    "    con = duckdb.connect()\n",
    "    con.execute(f\"PRAGMA memory_limit='{MEM_LIMIT_GB}GB';\")\n",
    "    con.execute(f\"PRAGMA temp_directory='{TEMP_DIR}';\")\n",
    "\n",
    "    # build query\n",
    "    query = f\"\"\"\n",
    "    COPY (\n",
    "        WITH r AS (\n",
    "            SELECT\n",
    "                noteId,\n",
    "                raterParticipantId,\n",
    "                createdAtMillis,\n",
    "                helpful,\n",
    "                notHelpful\n",
    "            FROM read_parquet('{ratings_dir}\\\\noteRatings-*.parquet')\n",
    "            LIMIT {BATCH_SIZE} OFFSET {offset}\n",
    "        ),\n",
    "        r2 AS (\n",
    "            SELECT\n",
    "                *,\n",
    "                CASE\n",
    "                    WHEN helpful = 1 THEN 'HELPFUL'\n",
    "                    WHEN notHelpful = 1 THEN 'NOT_HELPFUL'\n",
    "                    ELSE 'UNKNOWN'\n",
    "                END AS ratingStatus\n",
    "            FROM r\n",
    "        )\n",
    "        SELECT\n",
    "            r2.raterParticipantId,\n",
    "            r2.noteId,\n",
    "            r2.ratingStatus,\n",
    "            r2.createdAtMillis,\n",
    "            s.coreNoteIntercept,\n",
    "            s.coreNoteFactor1\n",
    "        FROM r2\n",
    "        LEFT JOIN read_parquet('{scores_path}') s USING (noteId)\n",
    "    ) TO '{out_part}' (FORMAT PARQUET);\n",
    "    \"\"\"\n",
    "\n",
    "    con.execute(query)\n",
    "    con.close()\n",
    "    part_files.append(out_part)\n",
    "    offset += BATCH_SIZE\n",
    "    gc.collect()\n",
    "\n",
    "# --- Merge all parts into one big file ---\n",
    "print(\"\\nðŸ§© Combining all parts into one parquet...\")\n",
    "con = duckdb.connect()\n",
    "con.execute(f\"PRAGMA memory_limit='{MEM_LIMIT_GB}GB';\")\n",
    "con.execute(f\"PRAGMA temp_directory='{TEMP_DIR}';\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  SELECT * FROM read_parquet({part_files})\n",
    ") TO '{out_path}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "con.close()\n",
    "print(f\"âœ… Done. ratings_with_scores.parquet written to {out_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29a10b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Loading and analyzing: C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\\intermediates\\ratings_with_scores.parquet\n",
      "\n",
      "Total rows: 171,877,798\n",
      "\n",
      "Column names and types:\n",
      "       column_name column_type null  key default extra\n",
      "raterParticipantId     VARCHAR  YES None    None  None\n",
      "            noteId      BIGINT  YES None    None  None\n",
      "      ratingStatus     VARCHAR  YES None    None  None\n",
      "   createdAtMillis      BIGINT  YES None    None  None\n",
      " coreNoteIntercept       FLOAT  YES None    None  None\n",
      "   coreNoteFactor1       FLOAT  YES None    None  None\n",
      "\n",
      "Total columns: 6\n",
      "\n",
      "Preview of first 5 rows:\n",
      "                                  raterParticipantId               noteId  \\\n",
      "0  C784F04F26E124F4D6EC01658D8F5565005D3092741FB3...  1714589572200304695   \n",
      "1  C784F04F26E124F4D6EC01658D8F5565005D3092741FB3...  1727529309810655291   \n",
      "2  C784F04F26E124F4D6EC01658D8F5565005D3092741FB3...  1728563772543910115   \n",
      "3  C784F04F26E124F4D6EC01658D8F5565005D3092741FB3...  1730887409146175557   \n",
      "4  C784F04F26E124F4D6EC01658D8F5565005D3092741FB3...  1733821630449668329   \n",
      "\n",
      "  ratingStatus  createdAtMillis  coreNoteIntercept  coreNoteFactor1  \n",
      "0      UNKNOWN    1697637486068           0.363793         0.463297  \n",
      "1      UNKNOWN    1700865444499           0.535273        -0.186475  \n",
      "2      UNKNOWN    1701005059910           0.379202         0.462800  \n",
      "3      UNKNOWN    1701610674078           0.435190         0.358438  \n",
      "4      UNKNOWN    1702228844546           0.358810        -0.582001  \n"
     ]
    }
   ],
   "source": [
    "# Path to the master parquet file\n",
    "base = r\"C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\\intermediates\"\n",
    "master_path = os.path.join(base, \"ratings_with_scores.parquet\")\n",
    "\n",
    "print(f\"ðŸ“Š Loading and analyzing: {master_path}\\n\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Get row count\n",
    "row_count = con.execute(f\"SELECT COUNT(*) FROM '{master_path}'\").fetchone()[0]\n",
    "print(f\"Total rows: {row_count:,}\\n\")\n",
    "\n",
    "# Get column names and types\n",
    "print(\"Column names and types:\")\n",
    "schema = con.execute(f\"DESCRIBE SELECT * FROM '{master_path}'\").fetchdf()\n",
    "print(schema.to_string(index=False))\n",
    "print(f\"\\nTotal columns: {len(schema)}\\n\")\n",
    "\n",
    "# Get basic statistics\n",
    "print(\"Preview of first 5 rows:\")\n",
    "preview = con.execute(f\"SELECT * FROM '{master_path}' LIMIT 5\").fetchdf()\n",
    "print(preview)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3313270e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Enriching user_period_master with biweekly aggregates...\n",
      "\n",
      "Executing join and enrichment query...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13562c597af461e9544054622aeb9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Done! Enriched master dataset saved to:\n",
      "C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\\user_period_master_enriched.parquet\n",
      "\n",
      "ðŸ“Š Verification:\n",
      " total_rows  rows_with_aggregate_data  pct_with_aggregate_data  unique_users\n",
      "   20869319                         0                      0.0       1278621\n",
      "\n",
      "ðŸ“ Preview of enriched dataset:\n",
      "Empty DataFrame\n",
      "Columns: [user_id, period_start, ratings_given, notes_rated_unique, tweets_rated_unique, agree_rate, disagree_rate, helpful_rate, somewhat_helpful_rate, not_helpful_rate, avg_helpful_flags, avg_not_helpful_flags, notes_written, num_misleading, num_not_misleading, avg_trustworthySources, media_note_ratio, misleading_flag_sum, not_misleading_flag_sum, share_misleading, avg_flags_per_note, notes_with_status, notes_helpful, notes_not_helpful, notes_nmr, notes_locked, first_helpful, latest_helpful, latest_not_helpful, avg_days_to_first_nonNMR, avg_days_to_lock, share_helpful, share_not_helpful, records, avg_successfulRatingNeededToEarnIn, avg_days_since_state_change, avg_days_since_earnout, is_new_user, is_earned_in, is_at_risk, is_earned_out, has_ever_earned_out, is_core_population, avg_modelingGroup, notes_requested, unique_tweets_requested, with_source_link, share_with_link, helpful_count, not_helpful_count, unknown_count, helpful_ratio, not_helpful_ratio, avg_core_note_intercept, min_core_note_intercept, max_core_note_intercept, stddev_core_note_intercept, non_null_intercept_count, avg_core_note_factor1, min_core_note_factor1, max_core_note_factor1, stddev_core_note_factor1, non_null_factor1_count]\n",
      "Index: []\n",
      " total_rows  rows_with_aggregate_data  pct_with_aggregate_data  unique_users\n",
      "   20869319                         0                      0.0       1278621\n",
      "\n",
      "ðŸ“ Preview of enriched dataset:\n",
      "Empty DataFrame\n",
      "Columns: [user_id, period_start, ratings_given, notes_rated_unique, tweets_rated_unique, agree_rate, disagree_rate, helpful_rate, somewhat_helpful_rate, not_helpful_rate, avg_helpful_flags, avg_not_helpful_flags, notes_written, num_misleading, num_not_misleading, avg_trustworthySources, media_note_ratio, misleading_flag_sum, not_misleading_flag_sum, share_misleading, avg_flags_per_note, notes_with_status, notes_helpful, notes_not_helpful, notes_nmr, notes_locked, first_helpful, latest_helpful, latest_not_helpful, avg_days_to_first_nonNMR, avg_days_to_lock, share_helpful, share_not_helpful, records, avg_successfulRatingNeededToEarnIn, avg_days_since_state_change, avg_days_since_earnout, is_new_user, is_earned_in, is_at_risk, is_earned_out, has_ever_earned_out, is_core_population, avg_modelingGroup, notes_requested, unique_tweets_requested, with_source_link, share_with_link, helpful_count, not_helpful_count, unknown_count, helpful_ratio, not_helpful_ratio, avg_core_note_intercept, min_core_note_intercept, max_core_note_intercept, stddev_core_note_intercept, non_null_intercept_count, avg_core_note_factor1, min_core_note_factor1, max_core_note_factor1, stddev_core_note_factor1, non_null_factor1_count]\n",
      "Index: []\n",
      "\n",
      "âœ… Enrichment complete!\n",
      "\n",
      "âœ… Enrichment complete!\n"
     ]
    }
   ],
   "source": [
    "import os, duckdb\n",
    "\n",
    "# --- Paths ---\n",
    "base = r\"C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\"\n",
    "master_path = os.path.join(base, \"user_period_master.parquet\")\n",
    "aggregates_path = os.path.join(base, \"intermediates\", \"user_biweekly_aggregates.parquet\")\n",
    "output_path = os.path.join(base, \"user_period_master_enriched.parquet\")\n",
    "\n",
    "print(\"ðŸš€ Enriching user_period_master with biweekly aggregates...\\n\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"PRAGMA memory_limit='8GB';\")\n",
    "\n",
    "# Join and enrich the master dataset\n",
    "query = f\"\"\"\n",
    "COPY (\n",
    "    SELECT \n",
    "        m.*,\n",
    "        -- Add biweekly aggregate columns\n",
    "        a.helpful_count,\n",
    "        a.not_helpful_count,\n",
    "        a.unknown_count,\n",
    "        a.helpful_ratio,\n",
    "        a.not_helpful_ratio,\n",
    "        a.avg_core_note_intercept,\n",
    "        a.min_core_note_intercept,\n",
    "        a.max_core_note_intercept,\n",
    "        a.stddev_core_note_intercept,\n",
    "        a.non_null_intercept_count,\n",
    "        a.avg_core_note_factor1,\n",
    "        a.min_core_note_factor1,\n",
    "        a.max_core_note_factor1,\n",
    "        a.stddev_core_note_factor1,\n",
    "        a.non_null_factor1_count\n",
    "    FROM read_parquet('{master_path}') m\n",
    "    LEFT JOIN read_parquet('{aggregates_path}') a \n",
    "        ON m.user_id = a.raterParticipantId \n",
    "        AND m.period_start = a.period_start\n",
    "    ORDER BY m.user_id, m.period_start\n",
    ") TO '{output_path}' (FORMAT PARQUET);\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing join and enrichment query...\")\n",
    "con.execute(query)\n",
    "\n",
    "print(f\"\\nâœ… Done! Enriched master dataset saved to:\\n{output_path}\")\n",
    "\n",
    "# Verify the join\n",
    "print(\"\\nðŸ“Š Verification:\")\n",
    "stats = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(helpful_count) as rows_with_aggregate_data,\n",
    "        ROUND(COUNT(helpful_count) * 100.0 / COUNT(*), 2) as pct_with_aggregate_data,\n",
    "        COUNT(DISTINCT user_id) as unique_users\n",
    "    FROM '{output_path}'\n",
    "\"\"\").fetchdf()\n",
    "print(stats.to_string(index=False))\n",
    "\n",
    "# Preview the enriched data\n",
    "print(\"\\nðŸ“ Preview of enriched dataset:\")\n",
    "preview = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM '{output_path}'\n",
    "    WHERE helpful_count IS NOT NULL\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "print(preview.to_string(index=False))\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(\"\\nâœ… Enrichment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e76949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” DEBUGGING JOIN ISSUE\n",
      "\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ user_period_master.parquet columns:\n",
      " column_name column_type null  key default extra\n",
      "     user_id     VARCHAR  YES None    None  None\n",
      "period_start   TIMESTAMP  YES None    None  None\n",
      "\n",
      "2ï¸âƒ£ user_biweekly_aggregates.parquet columns:\n",
      "       column_name column_type null  key default extra\n",
      "raterParticipantId     VARCHAR  YES None    None  None\n",
      "      period_start   TIMESTAMP  YES None    None  None\n",
      "\n",
      "3ï¸âƒ£ Sample values from user_period_master:\n",
      "                                                         user_id period_start user_id_type period_type\n",
      "313F5490745F282A1FA08CC181256F146CDF7D42015DE340FFB63168F3C56A30   2024-05-09      VARCHAR   TIMESTAMP\n",
      "4779F711D987B4892A4ECF2AE87CFF6F28D7E382C74840A7126DD578442896C1   2024-03-28      VARCHAR   TIMESTAMP\n",
      "132977B5FE5A50F4E81664A29B5157F8D88BF654365C97E2DDE9423137571D29   2025-02-27      VARCHAR   TIMESTAMP\n",
      "033B90CBECD732A561E1FC52BA1C2FA651C221D64DDCE4D1BD4ED8824EB16E69   2024-11-07      VARCHAR   TIMESTAMP\n",
      "060A613DEA82DF5D8CD8690CF1C76D07713AF32E6291AC6377D6D5699102E7D9   2024-08-01      VARCHAR   TIMESTAMP\n",
      "\n",
      "4ï¸âƒ£ Sample values from user_biweekly_aggregates:\n",
      "                                              raterParticipantId period_start user_id_type period_type\n",
      "0000010BB832A9CFDF102BF7B66896FA987C80FBB61EF6C4B04D875B85C07BD8   2023-12-31      VARCHAR   TIMESTAMP\n",
      "0000010BB832A9CFDF102BF7B66896FA987C80FBB61EF6C4B04D875B85C07BD8   2024-02-25      VARCHAR   TIMESTAMP\n",
      "0000010BB832A9CFDF102BF7B66896FA987C80FBB61EF6C4B04D875B85C07BD8   2024-03-24      VARCHAR   TIMESTAMP\n",
      "000011269AD6F327AED0F4086A732B4052F9D28E8791E117DC07497FF7CFB18C   2023-11-05      VARCHAR   TIMESTAMP\n",
      "000011269AD6F327AED0F4086A732B4052F9D28E8791E117DC07497FF7CFB18C   2023-12-03      VARCHAR   TIMESTAMP\n",
      "\n",
      "5ï¸âƒ£ Checking for user_id overlap:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51f490ea55a4cafaaf206cbbabf8ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " master_users  agg_users  overlapping_users\n",
      "      1278621    1278615            1278615\n",
      "\n",
      "6ï¸âƒ£ Checking period_start values:\n",
      "Master periods:\n",
      "min_period max_period  unique_periods\n",
      "2022-12-22 2025-09-25              73\n",
      "\n",
      "Aggregate periods:\n",
      "min_period max_period  unique_periods\n",
      "2023-01-01 2025-09-21              72\n",
      "\n",
      "7ï¸âƒ£ Manual join test (checking if ANY rows match):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad26fadba8d84ce2a0a3e42900e66712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " matching_rows\n",
      "             0\n",
      "\n",
      "================================================================================\n",
      "ðŸ” Debug complete! Check the output above to identify the mismatch.\n",
      "\n",
      "================================================================================\n",
      "ðŸ” Debug complete! Check the output above to identify the mismatch.\n"
     ]
    }
   ],
   "source": [
    "import os, duckdb\n",
    "\n",
    "# Debug: Check why join is failing\n",
    "base = r\"C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\"\n",
    "master_path = os.path.join(base, \"user_period_master.parquet\")\n",
    "aggregates_path = os.path.join(base, \"intermediates\", \"user_biweekly_aggregates.parquet\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "print(\"ðŸ” DEBUGGING JOIN ISSUE\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check column names and types in master\n",
    "print(\"\\n1ï¸âƒ£ user_period_master.parquet columns:\")\n",
    "master_schema = con.execute(f\"DESCRIBE SELECT * FROM '{master_path}'\").fetchdf()\n",
    "print(master_schema[master_schema['column_name'].isin(['user_id', 'period_start'])].to_string(index=False))\n",
    "\n",
    "# 2. Check column names and types in aggregates\n",
    "print(\"\\n2ï¸âƒ£ user_biweekly_aggregates.parquet columns:\")\n",
    "agg_schema = con.execute(f\"DESCRIBE SELECT * FROM '{aggregates_path}'\").fetchdf()\n",
    "print(agg_schema[agg_schema['column_name'].isin(['raterParticipantId', 'period_start'])].to_string(index=False))\n",
    "\n",
    "# 3. Sample values from master\n",
    "print(\"\\n3ï¸âƒ£ Sample values from user_period_master:\")\n",
    "master_sample = con.execute(f\"\"\"\n",
    "    SELECT user_id, period_start, typeof(user_id) as user_id_type, typeof(period_start) as period_type\n",
    "    FROM '{master_path}' \n",
    "    LIMIT 5\n",
    "\"\"\").fetchdf()\n",
    "print(master_sample.to_string(index=False))\n",
    "\n",
    "# 4. Sample values from aggregates\n",
    "print(\"\\n4ï¸âƒ£ Sample values from user_biweekly_aggregates:\")\n",
    "agg_sample = con.execute(f\"\"\"\n",
    "    SELECT raterParticipantId, period_start, typeof(raterParticipantId) as user_id_type, typeof(period_start) as period_type\n",
    "    FROM '{aggregates_path}' \n",
    "    LIMIT 5\n",
    "\"\"\").fetchdf()\n",
    "print(agg_sample.to_string(index=False))\n",
    "\n",
    "# 5. Check for overlap\n",
    "print(\"\\n5ï¸âƒ£ Checking for user_id overlap:\")\n",
    "overlap = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT m.user_id) as master_users,\n",
    "        COUNT(DISTINCT a.raterParticipantId) as agg_users,\n",
    "        COUNT(DISTINCT CASE WHEN a.raterParticipantId IS NOT NULL THEN m.user_id END) as overlapping_users\n",
    "    FROM '{master_path}' m\n",
    "    LEFT JOIN '{aggregates_path}' a ON m.user_id = a.raterParticipantId\n",
    "\"\"\").fetchdf()\n",
    "print(overlap.to_string(index=False))\n",
    "\n",
    "# 6. Check period_start overlap\n",
    "print(\"\\n6ï¸âƒ£ Checking period_start values:\")\n",
    "master_periods = con.execute(f\"\"\"\n",
    "    SELECT MIN(period_start) as min_period, MAX(period_start) as max_period, COUNT(DISTINCT period_start) as unique_periods\n",
    "    FROM '{master_path}'\n",
    "\"\"\").fetchdf()\n",
    "print(\"Master periods:\")\n",
    "print(master_periods.to_string(index=False))\n",
    "\n",
    "agg_periods = con.execute(f\"\"\"\n",
    "    SELECT MIN(period_start) as min_period, MAX(period_start) as max_period, COUNT(DISTINCT period_start) as unique_periods\n",
    "    FROM '{aggregates_path}'\n",
    "\"\"\").fetchdf()\n",
    "print(\"\\nAggregate periods:\")\n",
    "print(agg_periods.to_string(index=False))\n",
    "\n",
    "# 7. Try a manual join test\n",
    "print(\"\\n7ï¸âƒ£ Manual join test (checking if ANY rows match):\")\n",
    "join_test = con.execute(f\"\"\"\n",
    "    SELECT COUNT(*) as matching_rows\n",
    "    FROM '{master_path}' m\n",
    "    INNER JOIN '{aggregates_path}' a \n",
    "        ON m.user_id = a.raterParticipantId \n",
    "        AND m.period_start = a.period_start\n",
    "\"\"\").fetchdf()\n",
    "print(join_test.to_string(index=False))\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ” Debug complete! Check the output above to identify the mismatch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3304c468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” CHECKING DATA OVERLAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ Sample user IDs from user_period_master:\n",
      "['B96DBA35865FF3ACD48CE1B3E71E46BEB5342A1528953D3CEE7369E65D9B9BE4', 'BDDD6DB726B0B6FF2BB8B718778703C815E13DE6EC971B1F83233213E2D2E75E', 'BE7384DA2B162F0025C298872A68393CAEFDE803EE3DE74FBF17D1D6933C68A2', 'C16FFFAFD367FC48206B552A9496A436C6D7933FA2CFE6379777F3807DF9EE1F', 'C27C4C890FE835711E9BF5659A6371A798FCCCEDC6C712678696F343A924A7BF', 'C3F8C7BFD95EC6B1C0B77AFCE71FFCAB1784822BAFEB9272A2469D1EC1F91D0C', 'CD1E7944DE5F28754EF7FCC6D448E8265206E491C51BA870E6D7691AFD5CAECB', 'CF4E83EE1193644593E348E6EA8DCAC013D4EF98D1FBB7F7C9350FB19F222AC0', 'D3C370C2ED5561692BBD7CFCBEB04AAFCA9CE83E25DA1745B96E126538212479', 'D72D2BDFFE09B8CD8F31E6C696F8ED22D97E289A38E17A39CE42A809B3C3AF8D']\n",
      "\n",
      "2ï¸âƒ£ Sample user IDs from user_biweekly_aggregates:\n",
      "['077228CC77E5F3408DD196BBBD2C4A6EC646E7FAFA75C4B023A90347463989D3', '07728C139050BD52E936E4934624D82703E53B6D8C824C4699DD5E7D9BE3F829', '0773CDBE39E914769C5EC15863D91E502E0ABE8331AC7551BBA0C130F8950A7C', '077455602CD51743103E0F0D9C60B01226D98DF01CBBBDD77691813D122C64F1', '0774CFC971601565E0BE078EC0E87BB15EFCF8BDA372B52B14BC7BEE8A052096', '07768B6B36FF6CC4AF3D5655B0F7BD5D996F4B6897995000322DC4C29DF34E80', '07779A74D970ED21E3CA98E66BBBC27DBD38C0ED4A1B29C0698D6357660CD3CD', '0777C3A0863E38C0DB81A3CDF98FA518CD5020F0A5632FE03F95BBCE4AB76CE9', '0778A547F7512EE7887AD8FBACA7AA483CA37D45B17D1066D42AA61289DB5CB1', '07795FDC4053F8A14C10A91CBC98ED142C70FA6089D207E60E1CAB99346852A9']\n",
      "\n",
      "3ï¸âƒ£ Checking if ANY master users exist in aggregates:\n",
      "['B96DBA35865FF3ACD48CE1B3E71E46BEB5342A1528953D3CEE7369E65D9B9BE4', 'BDDD6DB726B0B6FF2BB8B718778703C815E13DE6EC971B1F83233213E2D2E75E', 'BE7384DA2B162F0025C298872A68393CAEFDE803EE3DE74FBF17D1D6933C68A2', 'C16FFFAFD367FC48206B552A9496A436C6D7933FA2CFE6379777F3807DF9EE1F', 'C27C4C890FE835711E9BF5659A6371A798FCCCEDC6C712678696F343A924A7BF', 'C3F8C7BFD95EC6B1C0B77AFCE71FFCAB1784822BAFEB9272A2469D1EC1F91D0C', 'CD1E7944DE5F28754EF7FCC6D448E8265206E491C51BA870E6D7691AFD5CAECB', 'CF4E83EE1193644593E348E6EA8DCAC013D4EF98D1FBB7F7C9350FB19F222AC0', 'D3C370C2ED5561692BBD7CFCBEB04AAFCA9CE83E25DA1745B96E126538212479', 'D72D2BDFFE09B8CD8F31E6C696F8ED22D97E289A38E17A39CE42A809B3C3AF8D']\n",
      "\n",
      "2ï¸âƒ£ Sample user IDs from user_biweekly_aggregates:\n",
      "['077228CC77E5F3408DD196BBBD2C4A6EC646E7FAFA75C4B023A90347463989D3', '07728C139050BD52E936E4934624D82703E53B6D8C824C4699DD5E7D9BE3F829', '0773CDBE39E914769C5EC15863D91E502E0ABE8331AC7551BBA0C130F8950A7C', '077455602CD51743103E0F0D9C60B01226D98DF01CBBBDD77691813D122C64F1', '0774CFC971601565E0BE078EC0E87BB15EFCF8BDA372B52B14BC7BEE8A052096', '07768B6B36FF6CC4AF3D5655B0F7BD5D996F4B6897995000322DC4C29DF34E80', '07779A74D970ED21E3CA98E66BBBC27DBD38C0ED4A1B29C0698D6357660CD3CD', '0777C3A0863E38C0DB81A3CDF98FA518CD5020F0A5632FE03F95BBCE4AB76CE9', '0778A547F7512EE7887AD8FBACA7AA483CA37D45B17D1066D42AA61289DB5CB1', '07795FDC4053F8A14C10A91CBC98ED142C70FA6089D207E60E1CAB99346852A9']\n",
      "\n",
      "3ï¸âƒ£ Checking if ANY master users exist in aggregates:\n",
      " sample_master_users  matching_in_aggregates\n",
      "                 100                     100\n",
      "\n",
      "4ï¸âƒ£ Checking if ANY aggregate users exist in master:\n",
      " sample_master_users  matching_in_aggregates\n",
      "                 100                     100\n",
      "\n",
      "4ï¸âƒ£ Checking if ANY aggregate users exist in master:\n",
      " sample_agg_users  matching_in_master\n",
      "              100                 100\n",
      "\n",
      "5ï¸âƒ£ Checking period_start formats (first few rows):\n",
      "Master periods:\n",
      "period_start      type\n",
      "  2022-12-22 TIMESTAMP\n",
      "  2023-01-05 TIMESTAMP\n",
      "  2023-01-19 TIMESTAMP\n",
      "  2023-02-02 TIMESTAMP\n",
      "  2023-02-16 TIMESTAMP\n",
      "\n",
      "Aggregate periods:\n",
      "period_start      type\n",
      "  2023-01-01 TIMESTAMP\n",
      "  2023-01-15 TIMESTAMP\n",
      "  2023-01-29 TIMESTAMP\n",
      "  2023-02-12 TIMESTAMP\n",
      "  2023-02-26 TIMESTAMP\n",
      " sample_agg_users  matching_in_master\n",
      "              100                 100\n",
      "\n",
      "5ï¸âƒ£ Checking period_start formats (first few rows):\n",
      "Master periods:\n",
      "period_start      type\n",
      "  2022-12-22 TIMESTAMP\n",
      "  2023-01-05 TIMESTAMP\n",
      "  2023-01-19 TIMESTAMP\n",
      "  2023-02-02 TIMESTAMP\n",
      "  2023-02-16 TIMESTAMP\n",
      "\n",
      "Aggregate periods:\n",
      "period_start      type\n",
      "  2023-01-01 TIMESTAMP\n",
      "  2023-01-15 TIMESTAMP\n",
      "  2023-01-29 TIMESTAMP\n",
      "  2023-02-12 TIMESTAMP\n",
      "  2023-02-26 TIMESTAMP\n",
      "\n",
      "================================================================================\n",
      "ðŸ’¡ If 'matching_in_aggregates' and 'matching_in_master' are both 0,\n",
      "   then the two files contain DIFFERENT sets of users!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ðŸ’¡ If 'matching_in_aggregates' and 'matching_in_master' are both 0,\n",
      "   then the two files contain DIFFERENT sets of users!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os, duckdb\n",
    "\n",
    "# Additional debugging: Check actual data overlap\n",
    "base = r\"C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\"\n",
    "master_path = os.path.join(base, \"user_period_master.parquet\")\n",
    "aggregates_path = os.path.join(base, \"intermediates\", \"user_biweekly_aggregates.parquet\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "print(\"ðŸ” CHECKING DATA OVERLAP\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Sample actual user IDs from both files\n",
    "print(\"\\n1ï¸âƒ£ Sample user IDs from user_period_master:\")\n",
    "master_users = con.execute(f\"\"\"\n",
    "    SELECT DISTINCT user_id \n",
    "    FROM '{master_path}' \n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "print(master_users['user_id'].tolist())\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Sample user IDs from user_biweekly_aggregates:\")\n",
    "agg_users = con.execute(f\"\"\"\n",
    "    SELECT DISTINCT raterParticipantId \n",
    "    FROM '{aggregates_path}' \n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "print(agg_users['raterParticipantId'].tolist())\n",
    "\n",
    "# 3. Check if ANY of the master users exist in aggregates\n",
    "print(\"\\n3ï¸âƒ£ Checking if ANY master users exist in aggregates:\")\n",
    "sample_overlap = con.execute(f\"\"\"\n",
    "    WITH master_sample AS (\n",
    "        SELECT DISTINCT user_id FROM '{master_path}' LIMIT 100\n",
    "    )\n",
    "    SELECT \n",
    "        COUNT(DISTINCT m.user_id) as sample_master_users,\n",
    "        COUNT(DISTINCT a.raterParticipantId) as matching_in_aggregates\n",
    "    FROM master_sample m\n",
    "    LEFT JOIN '{aggregates_path}' a ON m.user_id = a.raterParticipantId\n",
    "    WHERE a.raterParticipantId IS NOT NULL\n",
    "\"\"\").fetchdf()\n",
    "print(sample_overlap.to_string(index=False))\n",
    "\n",
    "# 4. Check if ANY of the aggregate users exist in master\n",
    "print(\"\\n4ï¸âƒ£ Checking if ANY aggregate users exist in master:\")\n",
    "reverse_overlap = con.execute(f\"\"\"\n",
    "    WITH agg_sample AS (\n",
    "        SELECT DISTINCT raterParticipantId FROM '{aggregates_path}' LIMIT 100\n",
    "    )\n",
    "    SELECT \n",
    "        COUNT(DISTINCT a.raterParticipantId) as sample_agg_users,\n",
    "        COUNT(DISTINCT m.user_id) as matching_in_master\n",
    "    FROM agg_sample a\n",
    "    LEFT JOIN '{master_path}' m ON a.raterParticipantId = m.user_id\n",
    "    WHERE m.user_id IS NOT NULL\n",
    "\"\"\").fetchdf()\n",
    "print(reverse_overlap.to_string(index=False))\n",
    "\n",
    "# 5. Check if it's a timestamp format issue\n",
    "print(\"\\n5ï¸âƒ£ Checking period_start formats (first few rows):\")\n",
    "master_periods_sample = con.execute(f\"\"\"\n",
    "    SELECT DISTINCT period_start, typeof(period_start) as type\n",
    "    FROM '{master_path}' \n",
    "    ORDER BY period_start\n",
    "    LIMIT 5\n",
    "\"\"\").fetchdf()\n",
    "print(\"Master periods:\")\n",
    "print(master_periods_sample.to_string(index=False))\n",
    "\n",
    "agg_periods_sample = con.execute(f\"\"\"\n",
    "    SELECT DISTINCT period_start, typeof(period_start) as type\n",
    "    FROM '{aggregates_path}' \n",
    "    ORDER BY period_start\n",
    "    LIMIT 5\n",
    "\"\"\").fetchdf()\n",
    "print(\"\\nAggregate periods:\")\n",
    "print(agg_periods_sample.to_string(index=False))\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ’¡ If 'matching_in_aggregates' and 'matching_in_master' are both 0,\")\n",
    "print(\"   then the two files contain DIFFERENT sets of users!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22782578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, duckdb\n",
    "\n",
    "# Focus on period_start matching\n",
    "base = r\"C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_data_aggregating\\data\"\n",
    "master_path = os.path.join(base, \"user_period_master.parquet\")\n",
    "aggregates_path = os.path.join(base, \"intermediates\", \"user_biweekly_aggregates.parquet\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "print(\"ðŸ” CHECKING PERIOD_START OVERLAP IN DETAIL\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Get actual period_start values from master\n",
    "print(\"\\n1ï¸âƒ£ Sample period_start values from user_period_master:\")\n",
    "master_periods = con.execute(f\"\"\"\n",
    "    SELECT DISTINCT period_start \n",
    "    FROM '{master_path}' \n",
    "    ORDER BY period_start\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "print(master_periods['period_start'].tolist())\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Sample period_start values from user_biweekly_aggregates:\")\n",
    "agg_periods = con.execute(f\"\"\"\n",
    "    SELECT DISTINCT period_start \n",
    "    FROM '{aggregates_path}' \n",
    "    ORDER BY period_start\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "print(agg_periods['period_start'].tolist())\n",
    "\n",
    "# 3. Check if ANY periods overlap\n",
    "print(\"\\n3ï¸âƒ£ Checking if ANY periods from master exist in aggregates:\")\n",
    "period_overlap = con.execute(f\"\"\"\n",
    "    WITH master_periods AS (\n",
    "        SELECT DISTINCT period_start FROM '{master_path}'\n",
    "    ),\n",
    "    agg_periods AS (\n",
    "        SELECT DISTINCT period_start FROM '{aggregates_path}'\n",
    "    )\n",
    "    SELECT \n",
    "        COUNT(DISTINCT m.period_start) as master_unique_periods,\n",
    "        COUNT(DISTINCT a.period_start) as agg_unique_periods,\n",
    "        COUNT(DISTINCT CASE WHEN a.period_start IS NOT NULL THEN m.period_start END) as overlapping_periods\n",
    "    FROM master_periods m\n",
    "    LEFT JOIN agg_periods a ON m.period_start = a.period_start\n",
    "\"\"\").fetchdf()\n",
    "print(period_overlap.to_string(index=False))\n",
    "\n",
    "# 4. Check for exact timestamp matching on a specific user\n",
    "print(\"\\n4ï¸âƒ£ Testing join on a specific user with period matching:\")\n",
    "specific_user_test = con.execute(f\"\"\"\n",
    "    WITH sample_user AS (\n",
    "        SELECT user_id FROM '{master_path}' LIMIT 1\n",
    "    )\n",
    "    SELECT \n",
    "        m.user_id,\n",
    "        m.period_start as master_period,\n",
    "        a.period_start as agg_period,\n",
    "        m.period_start = a.period_start as periods_match,\n",
    "        a.helpful_count\n",
    "    FROM '{master_path}' m\n",
    "    CROSS JOIN sample_user s\n",
    "    LEFT JOIN '{aggregates_path}' a \n",
    "        ON m.user_id = a.raterParticipantId \n",
    "        AND m.period_start = a.period_start\n",
    "    WHERE m.user_id = s.user_id\n",
    "    LIMIT 5\n",
    "\"\"\").fetchdf()\n",
    "print(specific_user_test.to_string(index=False))\n",
    "\n",
    "# 5. Check if there's a CAST issue with timestamps\n",
    "print(\"\\n5ï¸âƒ£ Testing with CAST to DATE:\")\n",
    "cast_test = con.execute(f\"\"\"\n",
    "    SELECT COUNT(*) as matching_rows_with_cast\n",
    "    FROM '{master_path}' m\n",
    "    INNER JOIN '{aggregates_path}' a \n",
    "        ON m.user_id = a.raterParticipantId \n",
    "        AND CAST(m.period_start AS DATE) = CAST(a.period_start AS DATE)\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "print(cast_test.to_string(index=False))\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ’¡ If overlapping_periods = 0, the period values don't match between files!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
