{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84cc3f01-ecd0-4511-a330-cce79bbccabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polars\n",
      "  Using cached polars-1.35.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting polars-runtime-32==1.35.1 (from polars)\n",
      "  Using cached polars_runtime_32-1.35.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Using cached polars-1.35.1-py3-none-any.whl (783 kB)\n",
      "Using cached polars_runtime_32-1.35.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.3 MB)\n",
      "Installing collected packages: polars-runtime-32, polars\n",
      "Successfully installed polars-1.35.1 polars-runtime-32-1.35.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting duckdb\n",
      "  Using cached duckdb-1.4.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (14 kB)\n",
      "Using cached duckdb-1.4.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (20.5 MB)\n",
      "Installing collected packages: duckdb\n",
      "Successfully installed duckdb-1.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install polars\n",
    "%pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f71f02-bb1e-402e-8f52-4154cbd4cda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classification values (printed one per line):\n",
      "\n",
      "â€¢ 'MISINFORMED_OR_POTENTIALLY_MISLEADING'\n",
      "â€¢ 'NOT_MISLEADING'\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load your raw notes dataset (the pre-aggregated one)\n",
    "path = \"/home/jovyan/Shared/2025-09-27-input/notes-00000.parquet\"\n",
    "df = pl.read_parquet(path)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af7ec76-e994-41f1-b77d-8fae0d8507c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Cleaning + aggregating Community Notes (2023+) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_274/3661600488.py:56: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  pl.count().alias(\"notes_written\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Aggregation complete â€” saved to:\n",
      "/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/notes_writers_2023_aggregated.parquet\n",
      "Rows: 1,028,703 | Columns: 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>period_start</th><th>notes_written</th><th>num_misleading</th><th>num_not_misleading</th><th>avg_trustworthySources</th><th>media_note_ratio</th><th>misleading_flag_sum</th><th>not_misleading_flag_sum</th><th>share_misleading</th><th>avg_flags_per_note</th></tr><tr><td>str</td><td>datetime[ms]</td><td>u32</td><td>u32</td><td>u32</td><td>f64</td><td>f64</td><td>list[i64]</td><td>list[i64]</td><td>f64</td><td>list[f64]</td></tr></thead><tbody><tr><td>&quot;000045A5FA0CF004F68CBF2913506Câ€¦</td><td>2023-03-30 00:00:00</td><td>5</td><td>5</td><td>0</td><td>1.0</td><td>0.0</td><td>[2, 1, â€¦ 1]</td><td>[0, 0, â€¦ 0]</td><td>1.0</td><td>[0.4, 0.2, â€¦ 0.2]</td></tr><tr><td>&quot;000045A5FA0CF004F68CBF2913506Câ€¦</td><td>2023-04-27 00:00:00</td><td>1</td><td>1</td><td>0</td><td>1.0</td><td>0.0</td><td>[2]</td><td>[0]</td><td>1.0</td><td>[2.0]</td></tr><tr><td>&quot;000045A5FA0CF004F68CBF2913506Câ€¦</td><td>2023-05-11 00:00:00</td><td>1</td><td>0</td><td>1</td><td>1.0</td><td>0.0</td><td>[0]</td><td>[1]</td><td>0.0</td><td>[1.0]</td></tr><tr><td>&quot;000045A5FA0CF004F68CBF2913506Câ€¦</td><td>2023-06-08 00:00:00</td><td>1</td><td>1</td><td>0</td><td>1.0</td><td>0.0</td><td>[2]</td><td>[0]</td><td>1.0</td><td>[2.0]</td></tr><tr><td>&quot;000045A5FA0CF004F68CBF2913506Câ€¦</td><td>2023-08-31 00:00:00</td><td>1</td><td>1</td><td>0</td><td>0.0</td><td>0.0</td><td>[3]</td><td>[0]</td><td>1.0</td><td>[3.0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 11)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ user_id   â”† period_st â”† notes_wri â”† num_misle â”† â€¦ â”† misleadin â”† not_misle â”† share_mis â”† avg_flag â”‚\n",
       "â”‚ ---       â”† art       â”† tten      â”† ading     â”†   â”† g_flag_su â”† ading_fla â”† leading   â”† s_per_no â”‚\n",
       "â”‚ str       â”† ---       â”† ---       â”† ---       â”†   â”† m         â”† g_sum     â”† ---       â”† te       â”‚\n",
       "â”‚           â”† datetime[ â”† u32       â”† u32       â”†   â”† ---       â”† ---       â”† f64       â”† ---      â”‚\n",
       "â”‚           â”† ms]       â”†           â”†           â”†   â”† list[i64] â”† list[i64] â”†           â”† list[f64 â”‚\n",
       "â”‚           â”†           â”†           â”†           â”†   â”†           â”†           â”†           â”† ]        â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 000045A5F â”† 2023-03-3 â”† 5         â”† 5         â”† â€¦ â”† [2, 1, â€¦  â”† [0, 0, â€¦  â”† 1.0       â”† [0.4,    â”‚\n",
       "â”‚ A0CF004F6 â”† 0         â”†           â”†           â”†   â”† 1]        â”† 0]        â”†           â”† 0.2, â€¦   â”‚\n",
       "â”‚ 8CBF29135 â”† 00:00:00  â”†           â”†           â”†   â”†           â”†           â”†           â”† 0.2]     â”‚\n",
       "â”‚ 06Câ€¦      â”†           â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 000045A5F â”† 2023-04-2 â”† 1         â”† 1         â”† â€¦ â”† [2]       â”† [0]       â”† 1.0       â”† [2.0]    â”‚\n",
       "â”‚ A0CF004F6 â”† 7         â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 8CBF29135 â”† 00:00:00  â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 06Câ€¦      â”†           â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 000045A5F â”† 2023-05-1 â”† 1         â”† 0         â”† â€¦ â”† [0]       â”† [1]       â”† 0.0       â”† [1.0]    â”‚\n",
       "â”‚ A0CF004F6 â”† 1         â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 8CBF29135 â”† 00:00:00  â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 06Câ€¦      â”†           â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 000045A5F â”† 2023-06-0 â”† 1         â”† 1         â”† â€¦ â”† [2]       â”† [0]       â”† 1.0       â”† [2.0]    â”‚\n",
       "â”‚ A0CF004F6 â”† 8         â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 8CBF29135 â”† 00:00:00  â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 06Câ€¦      â”†           â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 000045A5F â”† 2023-08-3 â”† 1         â”† 1         â”† â€¦ â”† [3]       â”† [0]       â”† 1.0       â”† [3.0]    â”‚\n",
       "â”‚ A0CF004F6 â”† 1         â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 8CBF29135 â”† 00:00:00  â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 06Câ€¦      â”†           â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "INPUT_PATH  = \"/home/jovyan/Shared/2025-09-27-input/notes-00000.parquet\"\n",
    "OUTPUT_DIR  = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core\"\n",
    "OUTPUT_NAME = \"notes_writers_2023_aggregated.parquet\"\n",
    "\n",
    "CUTOFF_DATE = datetime(2023, 1, 1)\n",
    "\n",
    "# =========================================================\n",
    "# PIPELINE\n",
    "# =========================================================\n",
    "print(\"ðŸš€ Cleaning + aggregating Community Notes (2023+) ...\")\n",
    "\n",
    "df = pl.scan_parquet(INPUT_PATH)\n",
    "\n",
    "# --- Rename to consistent naming ---\n",
    "df = df.rename({\n",
    "    \"noteAuthorParticipantId\": \"user_id\",   # âœ… your schema key\n",
    "    \"createdAtMillis\": \"created_at_ms\"\n",
    "})\n",
    "\n",
    "# --- Drop deprecated + metadata columns ---\n",
    "drop_cols = [\n",
    "    \"believable\", \"harmful\", \"validationDifficulty\",\n",
    "    \"_processing_commit_hash\", \"_processed_at\", \"_data_date\"\n",
    "]\n",
    "df = df.drop([c for c in drop_cols if c in df.collect_schema().names()])\n",
    "\n",
    "# --- Convert ms â†’ datetime ---\n",
    "df = df.with_columns([\n",
    "    pl.from_epoch(pl.col(\"created_at_ms\"), time_unit=\"ms\").alias(\"created_at\")\n",
    "])\n",
    "\n",
    "# --- Filter to 2023+ ---\n",
    "df = df.filter(pl.col(\"created_at\") >= pl.lit(CUTOFF_DATE))\n",
    "\n",
    "# --- Truncate to 14-day windows ---\n",
    "df = df.with_columns([\n",
    "    pl.col(\"created_at\").dt.truncate(\"14d\").alias(\"period_start\")\n",
    "])\n",
    "\n",
    "# --- Identify binary (checkbox) columns dynamically ---\n",
    "main_cols = [\n",
    "    \"noteId\", \"tweetId\", \"user_id\", \"created_at\", \"created_at_ms\",\n",
    "    \"summary\", \"classification\", \"period_start\"\n",
    "]\n",
    "binary_cols = [c for c in df.collect_schema().names() if c not in main_cols]\n",
    "\n",
    "# --- Aggregation expressions ---\n",
    "agg_exprs = [\n",
    "    pl.count().alias(\"notes_written\"),\n",
    "    (pl.col(\"classification\") == \"MISINFORMED_OR_POTENTIALLY_MISLEADING\")\n",
    "        .sum().alias(\"num_misleading\"),\n",
    "    (pl.col(\"classification\") == \"NOT_MISLEADING\")\n",
    "        .sum().alias(\"num_not_misleading\"),\n",
    "    (pl.col(\"trustworthySources\") == 1).mean().alias(\"avg_trustworthySources\"),\n",
    "    pl.col(\"isMediaNote\").mean().alias(\"media_note_ratio\"),\n",
    "]\n",
    "\n",
    "# --- Summed reasoning flags ---\n",
    "misleading_cols = [c for c in binary_cols if c.startswith(\"misleading\")]\n",
    "not_misleading_cols = [c for c in binary_cols if c.startswith(\"notMisleading\")]\n",
    "\n",
    "if misleading_cols:\n",
    "    agg_exprs.append(\n",
    "        pl.sum_horizontal([pl.col(c) for c in misleading_cols]).alias(\"misleading_flag_sum\")\n",
    "    )\n",
    "if not_misleading_cols:\n",
    "    agg_exprs.append(\n",
    "        pl.sum_horizontal([pl.col(c) for c in not_misleading_cols]).alias(\"not_misleading_flag_sum\")\n",
    "    )\n",
    "\n",
    "# --- Perform aggregation ---\n",
    "agg = (\n",
    "    df.group_by([\"user_id\", \"period_start\"])\n",
    "      .agg(agg_exprs)\n",
    "      .with_columns([\n",
    "          (pl.col(\"num_misleading\") / pl.col(\"notes_written\")).alias(\"share_misleading\"),\n",
    "          ((pl.col(\"misleading_flag_sum\") + pl.col(\"not_misleading_flag_sum\")) /\n",
    "           pl.col(\"notes_written\")).alias(\"avg_flags_per_note\")\n",
    "      ])\n",
    "      .sort([\"user_id\", \"period_start\"])\n",
    ")\n",
    "\n",
    "# --- Execute + Save ---\n",
    "result = agg.collect()\n",
    "out_path = os.path.join(OUTPUT_DIR, OUTPUT_NAME)\n",
    "result.write_parquet(out_path)\n",
    "\n",
    "print(f\"âœ… Aggregation complete â€” saved to:\\n{out_path}\")\n",
    "print(f\"Rows: {result.height:,} | Columns: {len(result.columns)}\")\n",
    "result.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69149f20-fc94-4494-9d05-98dfd566808e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Cleaning + aggregating Note Status History (2023+) ...\n",
      "âœ… Aggregation complete â€” saved to:\n",
      "/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/note_status_history_2023_aggregated.parquet\n",
      "Rows: 1,089,163 | Columns: 14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 14)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>period_start</th><th>notes_with_status</th><th>notes_helpful</th><th>notes_not_helpful</th><th>notes_nmr</th><th>notes_locked</th><th>first_helpful</th><th>latest_helpful</th><th>latest_not_helpful</th><th>avg_days_to_first_nonNMR</th><th>avg_days_to_lock</th><th>share_helpful</th><th>share_not_helpful</th></tr><tr><td>str</td><td>datetime[ms]</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;000045A5FA0CF004F68CBF2913506Câ€¦</td><td>2023-03-30 00:00:00</td><td>7</td><td>2</td><td>0</td><td>5</td><td>7</td><td>2</td><td>2</td><td>1</td><td>0.218733</td><td>29.193681</td><td>0.285714</td><td>0.0</td></tr><tr><td>&quot;000045A5FA0CF004F68CBF2913506Câ€¦</td><td>2023-04-27 00:00:00</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>null</td><td>14.00007</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;000045A5FA0CF004F68CBF2913506Câ€¦</td><td>2023-05-11 00:00:00</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>null</td><td>14.007234</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;000045A5FA0CF004F68CBF2913506Câ€¦</td><td>2023-06-08 00:00:00</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>null</td><td>14.032289</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;000045A5FA0CF004F68CBF2913506Câ€¦</td><td>2023-08-31 00:00:00</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>null</td><td>14.011199</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 14)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ user_id   â”† period_st â”† notes_wit â”† notes_hel â”† â€¦ â”† avg_days_ â”† avg_days_ â”† share_hel â”† share_no â”‚\n",
       "â”‚ ---       â”† art       â”† h_status  â”† pful      â”†   â”† to_first_ â”† to_lock   â”† pful      â”† t_helpfu â”‚\n",
       "â”‚ str       â”† ---       â”† ---       â”† ---       â”†   â”† nonNMR    â”† ---       â”† ---       â”† l        â”‚\n",
       "â”‚           â”† datetime[ â”† u32       â”† u32       â”†   â”† ---       â”† f64       â”† f64       â”† ---      â”‚\n",
       "â”‚           â”† ms]       â”†           â”†           â”†   â”† f64       â”†           â”†           â”† f64      â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 000045A5F â”† 2023-03-3 â”† 7         â”† 2         â”† â€¦ â”† 0.218733  â”† 29.193681 â”† 0.285714  â”† 0.0      â”‚\n",
       "â”‚ A0CF004F6 â”† 0         â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 8CBF29135 â”† 00:00:00  â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 06Câ€¦      â”†           â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 000045A5F â”† 2023-04-2 â”† 1         â”† 0         â”† â€¦ â”† null      â”† 14.00007  â”† 0.0       â”† 0.0      â”‚\n",
       "â”‚ A0CF004F6 â”† 7         â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 8CBF29135 â”† 00:00:00  â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 06Câ€¦      â”†           â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 000045A5F â”† 2023-05-1 â”† 1         â”† 0         â”† â€¦ â”† null      â”† 14.007234 â”† 0.0       â”† 0.0      â”‚\n",
       "â”‚ A0CF004F6 â”† 1         â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 8CBF29135 â”† 00:00:00  â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 06Câ€¦      â”†           â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 000045A5F â”† 2023-06-0 â”† 1         â”† 0         â”† â€¦ â”† null      â”† 14.032289 â”† 0.0       â”† 0.0      â”‚\n",
       "â”‚ A0CF004F6 â”† 8         â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 8CBF29135 â”† 00:00:00  â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 06Câ€¦      â”†           â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 000045A5F â”† 2023-08-3 â”† 1         â”† 0         â”† â€¦ â”† null      â”† 14.011199 â”† 0.0       â”† 0.0      â”‚\n",
       "â”‚ A0CF004F6 â”† 1         â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 8CBF29135 â”† 00:00:00  â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 06Câ€¦      â”†           â”†           â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "INPUT_PATH  = \"/home/jovyan/Shared/2025-09-27-input/noteStatusHistory-00000.parquet\"\n",
    "OUTPUT_DIR  = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core\"\n",
    "OUTPUT_NAME = \"note_status_history_2023_aggregated.parquet\"\n",
    "\n",
    "CUTOFF_DATE = datetime(2023, 1, 1)\n",
    "\n",
    "# =========================================================\n",
    "# PIPELINE\n",
    "# =========================================================\n",
    "print(\"ðŸš€ Cleaning + aggregating Note Status History (2023+) ...\")\n",
    "\n",
    "df = pl.scan_parquet(INPUT_PATH)\n",
    "\n",
    "# --- Rename to consistent naming ---\n",
    "df = df.rename({\n",
    "    \"noteAuthorParticipantId\": \"user_id\",\n",
    "    \"createdAtMillis\": \"created_at_ms\",\n",
    "    \"mostRecentNonNMRStatus\": \"latestNonNMRStatus\",\n",
    "})\n",
    "\n",
    "# --- Drop metadata columns ---\n",
    "drop_cols = [\n",
    "    \"timestampMinuteOfFinalScoringOutput\",\n",
    "    \"timestampMillisOfRetroLock\",\n",
    "    \"timestampMillisOfNmrDueToMinStableCrhTime\",\n",
    "    \"timestampMillisOfFirstNmrDueToMinStableCrhTime\",\n",
    "    \"_processing_commit_hash\",\n",
    "    \"_processed_at\",\n",
    "    \"_data_date\",\n",
    "]\n",
    "df = df.drop([c for c in drop_cols if c in df.collect_schema().names()])\n",
    "\n",
    "# --- Ensure timestamp columns are numeric ---\n",
    "timestamp_cols = [\n",
    "    \"created_at_ms\",\n",
    "    \"timestampMillisOfFirstNonNMRStatus\",\n",
    "    \"timestampMillisOfStatusLock\",\n",
    "]\n",
    "\n",
    "for col in timestamp_cols:\n",
    "    if col in df.collect_schema().names():\n",
    "        df = df.with_columns(pl.col(col).cast(pl.Int64, strict=False))\n",
    "\n",
    "\n",
    "# --- Convert ms â†’ datetime ---\n",
    "df = df.with_columns([\n",
    "    pl.from_epoch(pl.col(\"created_at_ms\"), time_unit=\"ms\").alias(\"created_at\")\n",
    "])\n",
    "\n",
    "# --- Filter to 2023+ only ---\n",
    "df = df.filter(pl.col(\"created_at\") >= pl.lit(CUTOFF_DATE))\n",
    "\n",
    "# --- Truncate to 14-day windows ---\n",
    "df = df.with_columns([\n",
    "    pl.col(\"created_at\").dt.truncate(\"14d\").alias(\"period_start\")\n",
    "])\n",
    "\n",
    "# --- Aggregations per user Ã— period ---\n",
    "agg_exprs = [\n",
    "    pl.len().alias(\"notes_with_status\"),\n",
    "    (pl.col(\"currentStatus\") == \"CURRENTLY_RATED_HELPFUL\")\n",
    "        .sum().alias(\"notes_helpful\"),\n",
    "    (pl.col(\"currentStatus\") == \"CURRENTLY_RATED_NOT_HELPFUL\")\n",
    "        .sum().alias(\"notes_not_helpful\"),\n",
    "    (pl.col(\"currentStatus\") == \"NEEDS_MORE_RATINGS\")\n",
    "        .sum().alias(\"notes_nmr\"),\n",
    "    (pl.col(\"lockedStatus\") != \"\").sum().alias(\"notes_locked\"),\n",
    "    (pl.col(\"firstNonNMRStatus\") == \"CURRENTLY_RATED_HELPFUL\")\n",
    "        .sum().alias(\"first_helpful\"),\n",
    "    (pl.col(\"latestNonNMRStatus\") == \"CURRENTLY_RATED_HELPFUL\")\n",
    "        .sum().alias(\"latest_helpful\"),\n",
    "    (pl.col(\"latestNonNMRStatus\") == \"CURRENTLY_RATED_NOT_HELPFUL\")\n",
    "        .sum().alias(\"latest_not_helpful\"),\n",
    "\n",
    "    # Timing metrics (average days)\n",
    "    ((pl.col(\"timestampMillisOfFirstNonNMRStatus\") - pl.col(\"created_at_ms\")) / (1000 * 60 * 60 * 24))\n",
    "        .mean().alias(\"avg_days_to_first_nonNMR\"),\n",
    "    ((pl.col(\"timestampMillisOfStatusLock\") - pl.col(\"created_at_ms\")) / (1000 * 60 * 60 * 24))\n",
    "        .mean().alias(\"avg_days_to_lock\"),\n",
    "]\n",
    "\n",
    "# --- Compute ratios ---\n",
    "agg = (\n",
    "    df.group_by([\"user_id\", \"period_start\"])\n",
    "      .agg(agg_exprs)\n",
    "      .with_columns([\n",
    "          (pl.col(\"notes_helpful\") / pl.col(\"notes_with_status\")).alias(\"share_helpful\"),\n",
    "          (pl.col(\"notes_not_helpful\") / pl.col(\"notes_with_status\")).alias(\"share_not_helpful\"),\n",
    "      ])\n",
    "      .sort([\"user_id\", \"period_start\"])\n",
    ")\n",
    "\n",
    "# --- Execute & save ---\n",
    "result = agg.collect()\n",
    "out_path = os.path.join(OUTPUT_DIR, OUTPUT_NAME)\n",
    "result.write_parquet(out_path)\n",
    "\n",
    "print(f\"âœ… Aggregation complete â€” saved to:\\n{out_path}\")\n",
    "print(f\"Rows: {result.height:,} | Columns: {len(result.columns)}\")\n",
    "result.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "515ce380-c7a4-446c-b1ec-c3c4cf19a4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Cleaning + aggregating User Enrollment (2023+) ...\n",
      "âœ… Aggregation complete â€” saved to:\n",
      "/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/user_enrollment_2023_aggregated.parquet\n",
      "Rows: 1,279,984 | Columns: 13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>period_start</th><th>records</th><th>avg_successfulRatingNeededToEarnIn</th><th>avg_days_since_state_change</th><th>avg_days_since_earnout</th><th>is_new_user</th><th>is_earned_in</th><th>is_at_risk</th><th>is_earned_out</th><th>has_ever_earned_out</th><th>is_core_population</th><th>avg_modelingGroup</th></tr><tr><td>str</td><td>datetime[ms]</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td><td>i8</td><td>i8</td><td>i8</td><td>i8</td><td>i8</td><td>i8</td><td>f64</td></tr></thead><tbody><tr><td>&quot;0000010BB832A9CFDF102BF7B66896â€¦</td><td>2024-03-28 00:00:00</td><td>1</td><td>5.0</td><td>583.021264</td><td>null</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>6.0</td></tr><tr><td>&quot;000011269AD6F327AED0F4086A732Bâ€¦</td><td>2024-03-28 00:00:00</td><td>1</td><td>5.0</td><td>583.021264</td><td>null</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>3.0</td></tr><tr><td>&quot;00001E2644DAE39EE4C52C373B921Dâ€¦</td><td>2025-07-31 00:00:00</td><td>1</td><td>5.0</td><td>96.346167</td><td>null</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>13.0</td></tr><tr><td>&quot;00002C7FD6E0080A69D0AB879C3D9Bâ€¦</td><td>2025-08-28 00:00:00</td><td>1</td><td>5.0</td><td>61.836681</td><td>null</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1.0</td></tr><tr><td>&quot;0000315D36021A528D85155729DDBFâ€¦</td><td>2024-10-10 00:00:00</td><td>1</td><td>5.0</td><td>380.346775</td><td>null</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>13.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 13)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ user_id    â”† period_st â”† records â”† avg_succe â”† â€¦ â”† is_earned â”† has_ever_ â”† is_core_p â”† avg_model â”‚\n",
       "â”‚ ---        â”† art       â”† ---     â”† ssfulRati â”†   â”† _out      â”† earned_ou â”† opulation â”† ingGroup  â”‚\n",
       "â”‚ str        â”† ---       â”† u32     â”† ngNeededT â”†   â”† ---       â”† t         â”† ---       â”† ---       â”‚\n",
       "â”‚            â”† datetime[ â”†         â”† oEaâ€¦      â”†   â”† i8        â”† ---       â”† i8        â”† f64       â”‚\n",
       "â”‚            â”† ms]       â”†         â”† ---       â”†   â”†           â”† i8        â”†           â”†           â”‚\n",
       "â”‚            â”†           â”†         â”† f64       â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 0000010BB8 â”† 2024-03-2 â”† 1       â”† 5.0       â”† â€¦ â”† 0         â”† 0         â”† 1         â”† 6.0       â”‚\n",
       "â”‚ 32A9CFDF10 â”† 8         â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 2BF7B66896 â”† 00:00:00  â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ â€¦          â”†           â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 000011269A â”† 2024-03-2 â”† 1       â”† 5.0       â”† â€¦ â”† 0         â”† 0         â”† 1         â”† 3.0       â”‚\n",
       "â”‚ D6F327AED0 â”† 8         â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ F4086A732B â”† 00:00:00  â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ â€¦          â”†           â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 00001E2644 â”† 2025-07-3 â”† 1       â”† 5.0       â”† â€¦ â”† 0         â”† 0         â”† 1         â”† 13.0      â”‚\n",
       "â”‚ DAE39EE4C5 â”† 1         â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 2C373B921D â”† 00:00:00  â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ â€¦          â”†           â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 00002C7FD6 â”† 2025-08-2 â”† 1       â”† 5.0       â”† â€¦ â”† 0         â”† 0         â”† 1         â”† 1.0       â”‚\n",
       "â”‚ E0080A69D0 â”† 8         â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ AB879C3D9B â”† 00:00:00  â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ â€¦          â”†           â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 0000315D36 â”† 2024-10-1 â”† 1       â”† 5.0       â”† â€¦ â”† 0         â”† 0         â”† 1         â”† 13.0      â”‚\n",
       "â”‚ 021A528D85 â”† 0         â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ 155729DDBF â”† 00:00:00  â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â”‚ â€¦          â”†           â”†         â”†           â”†   â”†           â”†           â”†           â”†           â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "INPUT_PATH  = \"/home/jovyan/Shared/2025-09-27-input/userEnrollment-00000.parquet\"\n",
    "OUTPUT_DIR  = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core\"\n",
    "OUTPUT_NAME = \"user_enrollment_2023_aggregated.parquet\"\n",
    "\n",
    "CUTOFF_DATE = datetime(2023, 1, 1)\n",
    "\n",
    "# =========================================================\n",
    "# PIPELINE\n",
    "# =========================================================\n",
    "print(\"ðŸš€ Cleaning + aggregating User Enrollment (2023+) ...\")\n",
    "\n",
    "df = pl.scan_parquet(INPUT_PATH)\n",
    "\n",
    "# --- Rename for consistency ---\n",
    "df = df.rename({\"participantId\": \"user_id\"})\n",
    "\n",
    "# --- Drop metadata columns ---\n",
    "drop_cols = [\"_processing_commit_hash\", \"_processed_at\", \"_data_date\"]\n",
    "df = df.drop([c for c in drop_cols if c in df.collect_schema().names()])\n",
    "\n",
    "# --- Ensure numeric timestamps ---\n",
    "timestamp_cols = [\"timestampOfLastStateChange\", \"timestampOfLastEarnOut\"]\n",
    "for col in timestamp_cols:\n",
    "    if col in df.collect_schema().names():\n",
    "        df = df.with_columns(pl.col(col).cast(pl.Int64, strict=False))\n",
    "\n",
    "# --- Convert timestamps to datetime (for filtering & binning) ---\n",
    "df = df.with_columns([\n",
    "    pl.from_epoch(pl.col(\"timestampOfLastStateChange\"), time_unit=\"ms\").alias(\"last_state_change_dt\")\n",
    "])\n",
    "\n",
    "# --- Filter to users active after 2023 ---\n",
    "df = df.filter(pl.col(\"last_state_change_dt\") >= pl.lit(CUTOFF_DATE))\n",
    "\n",
    "# --- Create 14-day period bins from last_state_change_dt ---\n",
    "df = df.with_columns([\n",
    "    pl.col(\"last_state_change_dt\").dt.truncate(\"14d\").alias(\"period_start\")\n",
    "])\n",
    "\n",
    "# --- Derived flags ---\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"enrollmentState\") == \"newUser\").cast(pl.Int8).alias(\"is_new_user\"),\n",
    "    (pl.col(\"enrollmentState\") == \"earnedIn\").cast(pl.Int8).alias(\"is_earned_in\"),\n",
    "    (pl.col(\"enrollmentState\") == \"atRisk\").cast(pl.Int8).alias(\"is_at_risk\"),\n",
    "    (pl.col(\"enrollmentState\").str.contains(\"earnedOut\")).cast(pl.Int8).alias(\"is_earned_out\"),\n",
    "    (pl.col(\"modelingPopulation\") == \"CORE\").cast(pl.Int8).alias(\"is_core_population\"),\n",
    "    (pl.col(\"timestampOfLastEarnOut\") != 1).cast(pl.Int8).alias(\"has_ever_earned_out\")\n",
    "])\n",
    "\n",
    "# --- Compute days since state changes ---\n",
    "current_time = pl.lit(datetime.now().timestamp() * 1000)\n",
    "df = df.with_columns([\n",
    "    ((current_time - pl.col(\"timestampOfLastStateChange\")) / (1000 * 60 * 60 * 24))\n",
    "        .alias(\"days_since_last_state_change\"),\n",
    "    (\n",
    "        pl.when(pl.col(\"timestampOfLastEarnOut\") != 1)\n",
    "        .then((current_time - pl.col(\"timestampOfLastEarnOut\")) / (1000 * 60 * 60 * 24))\n",
    "        .otherwise(None)\n",
    "        .alias(\"days_since_last_earnout\")\n",
    "    )\n",
    "])\n",
    "\n",
    "# --- Aggregate per user Ã— period ---\n",
    "agg = (\n",
    "    df.group_by([\"user_id\", \"period_start\"])\n",
    "      .agg([\n",
    "          pl.len().alias(\"records\"),  # modern replacement for count()\n",
    "          pl.mean(\"successfulRatingNeededToEarnIn\").alias(\"avg_successfulRatingNeededToEarnIn\"),\n",
    "          pl.mean(\"days_since_last_state_change\").alias(\"avg_days_since_state_change\"),\n",
    "          pl.mean(\"days_since_last_earnout\").alias(\"avg_days_since_earnout\"),\n",
    "          pl.max(\"is_new_user\").alias(\"is_new_user\"),\n",
    "          pl.max(\"is_earned_in\").alias(\"is_earned_in\"),\n",
    "          pl.max(\"is_at_risk\").alias(\"is_at_risk\"),\n",
    "          pl.max(\"is_earned_out\").alias(\"is_earned_out\"),\n",
    "          pl.max(\"has_ever_earned_out\").alias(\"has_ever_earned_out\"),\n",
    "          pl.max(\"is_core_population\").alias(\"is_core_population\"),\n",
    "          pl.mean(\"modelingGroup\").alias(\"avg_modelingGroup\"),\n",
    "      ])\n",
    "      .sort([\"user_id\", \"period_start\"])\n",
    ")\n",
    "\n",
    "# --- Execute + Save ---\n",
    "result = agg.collect()\n",
    "out_path = os.path.join(OUTPUT_DIR, OUTPUT_NAME)\n",
    "result.write_parquet(out_path)\n",
    "\n",
    "print(f\"âœ… Aggregation complete â€” saved to:\\n{out_path}\")\n",
    "print(f\"Rows: {result.height:,} | Columns: {len(result.columns)}\")\n",
    "result.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "613c17b8-c473-4f8b-924d-4b4e6b5ffd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Cleaning + aggregating Note Requests (2023+) ...\n",
      "âœ… Aggregation complete â€” saved to:\n",
      "/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/note_requests_2023_aggregated.parquet\n",
      "Rows: 3,817,344 | Columns: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>period_start</th><th>notes_requested</th><th>unique_tweets_requested</th><th>with_source_link</th><th>share_with_link</th></tr><tr><td>str</td><td>datetime[ms]</td><td>u32</td><td>u32</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;000004CC6A9EA228A4D367C463A49Bâ€¦</td><td>2024-08-29 00:00:00</td><td>1</td><td>1</td><td>0</td><td>0.0</td></tr><tr><td>&quot;00000D07B9A6256C0D44099CA726D3â€¦</td><td>2025-04-24 00:00:00</td><td>1</td><td>1</td><td>0</td><td>0.0</td></tr><tr><td>&quot;000017720184E7316E75A7875EB214â€¦</td><td>2025-07-03 00:00:00</td><td>2</td><td>2</td><td>2</td><td>1.0</td></tr><tr><td>&quot;00001E9887ECC2F2D27A2A8837939Bâ€¦</td><td>2024-09-26 00:00:00</td><td>1</td><td>1</td><td>0</td><td>0.0</td></tr><tr><td>&quot;00002C7FD6E0080A69D0AB879C3D9Bâ€¦</td><td>2024-11-21 00:00:00</td><td>1</td><td>1</td><td>0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ user_id        â”† period_start   â”† notes_requeste â”† unique_tweets â”† with_source_l â”† share_with_li â”‚\n",
       "â”‚ ---            â”† ---            â”† d              â”† _requested    â”† ink           â”† nk            â”‚\n",
       "â”‚ str            â”† datetime[ms]   â”† ---            â”† ---           â”† ---           â”† ---           â”‚\n",
       "â”‚                â”†                â”† u32            â”† u32           â”† i64           â”† f64           â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 000004CC6A9EA2 â”† 2024-08-29     â”† 1              â”† 1             â”† 0             â”† 0.0           â”‚\n",
       "â”‚ 28A4D367C463A4 â”† 00:00:00       â”†                â”†               â”†               â”†               â”‚\n",
       "â”‚ 9Bâ€¦            â”†                â”†                â”†               â”†               â”†               â”‚\n",
       "â”‚ 00000D07B9A625 â”† 2025-04-24     â”† 1              â”† 1             â”† 0             â”† 0.0           â”‚\n",
       "â”‚ 6C0D44099CA726 â”† 00:00:00       â”†                â”†               â”†               â”†               â”‚\n",
       "â”‚ D3â€¦            â”†                â”†                â”†               â”†               â”†               â”‚\n",
       "â”‚ 000017720184E7 â”† 2025-07-03     â”† 2              â”† 2             â”† 2             â”† 1.0           â”‚\n",
       "â”‚ 316E75A7875EB2 â”† 00:00:00       â”†                â”†               â”†               â”†               â”‚\n",
       "â”‚ 14â€¦            â”†                â”†                â”†               â”†               â”†               â”‚\n",
       "â”‚ 00001E9887ECC2 â”† 2024-09-26     â”† 1              â”† 1             â”† 0             â”† 0.0           â”‚\n",
       "â”‚ F2D27A2A883793 â”† 00:00:00       â”†                â”†               â”†               â”†               â”‚\n",
       "â”‚ 9Bâ€¦            â”†                â”†                â”†               â”†               â”†               â”‚\n",
       "â”‚ 00002C7FD6E008 â”† 2024-11-21     â”† 1              â”† 1             â”† 0             â”† 0.0           â”‚\n",
       "â”‚ 0A69D0AB879C3D â”† 00:00:00       â”†                â”†               â”†               â”†               â”‚\n",
       "â”‚ 9Bâ€¦            â”†                â”†                â”†               â”†               â”†               â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "INPUT_PATH  = \"/home/jovyan/Shared/2025-09-27-input/noteRequests-00000.parquet\"\n",
    "OUTPUT_DIR  = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core\"\n",
    "OUTPUT_NAME = \"note_requests_2023_aggregated.parquet\"\n",
    "\n",
    "CUTOFF_DATE = datetime(2023, 1, 1)\n",
    "\n",
    "# =========================================================\n",
    "# PIPELINE\n",
    "# =========================================================\n",
    "print(\"ðŸš€ Cleaning + aggregating Note Requests (2023+) ...\")\n",
    "\n",
    "df = pl.scan_parquet(INPUT_PATH)\n",
    "\n",
    "# --- Rename for consistency ---\n",
    "df = df.rename({\n",
    "    \"userId\": \"user_id\",\n",
    "    \"createdAtMillis\": \"created_at_ms\"\n",
    "})\n",
    "\n",
    "# --- Drop metadata columns if present ---\n",
    "drop_cols = [\"_processing_commit_hash\", \"_processed_at\", \"_data_date\"]\n",
    "df = df.drop([c for c in drop_cols if c in df.collect_schema().names()])\n",
    "\n",
    "# --- Ensure numeric timestamp ---\n",
    "if \"created_at_ms\" in df.collect_schema().names():\n",
    "    df = df.with_columns(pl.col(\"created_at_ms\").cast(pl.Int64, strict=False))\n",
    "\n",
    "# --- Convert to datetime ---\n",
    "df = df.with_columns([\n",
    "    pl.from_epoch(pl.col(\"created_at_ms\"), time_unit=\"ms\").alias(\"created_at\")\n",
    "])\n",
    "\n",
    "# --- Filter to 2023+ ---\n",
    "df = df.filter(pl.col(\"created_at\") >= pl.lit(CUTOFF_DATE))\n",
    "\n",
    "# --- 14-day bins ---\n",
    "df = df.with_columns([\n",
    "    pl.col(\"created_at\").dt.truncate(\"14d\").alias(\"period_start\")\n",
    "])\n",
    "\n",
    "# --- Derived flags ---\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"sourceLink\").str.strip_chars().is_not_null() & (pl.col(\"sourceLink\") != \"\"))\n",
    "        .cast(pl.Int8)\n",
    "        .alias(\"has_source_link\")\n",
    "])\n",
    "\n",
    "# --- Aggregations per user Ã— period ---\n",
    "agg = (\n",
    "    df.group_by([\"user_id\", \"period_start\"])\n",
    "      .agg([\n",
    "          pl.len().alias(\"notes_requested\"),\n",
    "          pl.n_unique(\"tweetId\").alias(\"unique_tweets_requested\"),\n",
    "          pl.sum(\"has_source_link\").alias(\"with_source_link\")\n",
    "      ])\n",
    "      .with_columns([\n",
    "          (pl.col(\"with_source_link\") / pl.col(\"notes_requested\")).alias(\"share_with_link\")\n",
    "      ])\n",
    "      .sort([\"user_id\", \"period_start\"])\n",
    ")\n",
    "\n",
    "# --- Execute + Save ---\n",
    "result = agg.collect()\n",
    "out_path = os.path.join(OUTPUT_DIR, OUTPUT_NAME)\n",
    "result.write_parquet(out_path)\n",
    "\n",
    "print(f\"âœ… Aggregation complete â€” saved to:\\n{out_path}\")\n",
    "print(f\"Rows: {result.height:,} | Columns: {len(result.columns)}\")\n",
    "result.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23980694-a4b0-4476-a0c4-83e6d1ef24d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files:\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "parquet_files = sorted(glob.glob(\"/home/jovyan/Shared/2025-09-27-input/noteRating-*.parquet\"))\n",
    "print(f\"Found {len(parquet_files)} files:\")\n",
    "for f in parquet_files[:5]:\n",
    "    print(\" \", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0552aa5-8c74-494c-9843-1c3c1b4cb2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‚ Processing noteRatings-00000.parquet ...\n",
      "  Rows in file: 14,115,918\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2687/1044297742.py:118: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n",
      "  .collect(streaming=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Batch 2 (2,000,000 rows)\n",
      "  ðŸ§© Batch 3 (2,000,000 rows)\n",
      "  ðŸ§© Batch 4 (2,000,000 rows)\n",
      "  ðŸ§© Batch 5 (2,000,000 rows)\n",
      "  ðŸ§© Batch 6 (2,000,000 rows)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e60eaf6724343398d5dbfd28d261c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Batch 7 (2,000,000 rows)\n",
      "  ðŸ§© Batch 8 (115,918 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:   5%|â–Œ         | 1/20 [00:19<06:08, 19.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00000.parquet (1,720,180 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00001.parquet ...\n",
      "  Rows in file: 14,037,156\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (2,000,000 rows)\n",
      "  ðŸ§© Batch 3 (2,000,000 rows)\n",
      "  ðŸ§© Batch 4 (2,000,000 rows)\n",
      "  ðŸ§© Batch 5 (2,000,000 rows)\n",
      "  ðŸ§© Batch 6 (2,000,000 rows)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32fdb82199a4935aaf07a5ba669681e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Batch 7 (2,000,000 rows)\n",
      "  ðŸ§© Batch 8 (37,156 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  10%|â–ˆ         | 2/20 [00:35<05:17, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00001.parquet (1,724,554 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00002.parquet ...\n",
      "  Rows in file: 14,014,554\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (2,000,000 rows)\n",
      "  ðŸ§© Batch 3 (2,000,000 rows)\n",
      "  ðŸ§© Batch 4 (2,000,000 rows)\n",
      "  ðŸ§© Batch 5 (2,000,000 rows)\n",
      "  ðŸ§© Batch 6 (2,000,000 rows)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe4d2f14d934e6f859059fb9cae90e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Batch 7 (2,000,000 rows)\n",
      "  ðŸ§© Batch 8 (14,554 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  15%|â–ˆâ–Œ        | 3/20 [00:52<04:49, 17.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00002.parquet (1,728,678 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00003.parquet ...\n",
      "  Rows in file: 14,020,408\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (2,000,000 rows)\n",
      "  ðŸ§© Batch 3 (2,000,000 rows)\n",
      "  ðŸ§© Batch 4 (2,000,000 rows)\n",
      "  ðŸ§© Batch 5 (2,000,000 rows)\n",
      "  ðŸ§© Batch 6 (2,000,000 rows)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af74a6fbff7409e8ddb6af0b9c9d9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Batch 7 (2,000,000 rows)\n",
      "  ðŸ§© Batch 8 (20,408 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  20%|â–ˆâ–ˆ        | 4/20 [01:09<04:34, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00003.parquet (1,730,174 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00004.parquet ...\n",
      "  Rows in file: 13,895,420\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (2,000,000 rows)\n",
      "  ðŸ§© Batch 3 (2,000,000 rows)\n",
      "  ðŸ§© Batch 4 (2,000,000 rows)\n",
      "  ðŸ§© Batch 5 (2,000,000 rows)\n",
      "  ðŸ§© Batch 6 (2,000,000 rows)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c486707c45cd4cb9b6bdd70a07cc90e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Batch 7 (1,895,420 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [01:24<04:06, 16.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00004.parquet (1,741,006 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00005.parquet ...\n",
      "  Rows in file: 13,879,060\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (2,000,000 rows)\n",
      "  ðŸ§© Batch 3 (2,000,000 rows)\n",
      "  ðŸ§© Batch 4 (2,000,000 rows)\n",
      "  ðŸ§© Batch 5 (2,000,000 rows)\n",
      "  ðŸ§© Batch 6 (2,000,000 rows)\n",
      "  ðŸ§© Batch 7 (1,879,060 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:40<03:45, 16.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00005.parquet (1,744,671 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00006.parquet ...\n",
      "  Rows in file: 13,785,568\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (2,000,000 rows)\n",
      "  ðŸ§© Batch 3 (2,000,000 rows)\n",
      "  ðŸ§© Batch 4 (2,000,000 rows)\n",
      "  ðŸ§© Batch 5 (2,000,000 rows)\n",
      "  ðŸ§© Batch 6 (2,000,000 rows)\n",
      "  ðŸ§© Batch 7 (1,785,568 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [01:55<03:26, 15.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00006.parquet (1,744,438 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00007.parquet ...\n",
      "  Rows in file: 13,836,663\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (2,000,000 rows)\n",
      "  ðŸ§© Batch 3 (2,000,000 rows)\n",
      "  ðŸ§© Batch 4 (2,000,000 rows)\n",
      "  ðŸ§© Batch 5 (2,000,000 rows)\n",
      "  ðŸ§© Batch 6 (2,000,000 rows)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec62c1cfcbd4e61b3b3227eaa02db38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Batch 7 (1,836,663 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [02:10<03:08, 15.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00007.parquet (1,740,474 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00008.parquet ...\n",
      "  Rows in file: 13,725,811\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (2,000,000 rows)\n",
      "  ðŸ§© Batch 3 (2,000,000 rows)\n",
      "  ðŸ§© Batch 4 (2,000,000 rows)\n",
      "  ðŸ§© Batch 5 (2,000,000 rows)\n",
      "  ðŸ§© Batch 6 (2,000,000 rows)\n",
      "  ðŸ§© Batch 7 (1,725,811 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [02:26<02:51, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00008.parquet (1,736,683 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00009.parquet ...\n",
      "  Rows in file: 13,771,655\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (2,000,000 rows)\n",
      "  ðŸ§© Batch 3 (2,000,000 rows)\n",
      "  ðŸ§© Batch 4 (2,000,000 rows)\n",
      "  ðŸ§© Batch 5 (2,000,000 rows)\n",
      "  ðŸ§© Batch 6 (2,000,000 rows)\n",
      "  ðŸ§© Batch 7 (1,771,655 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [02:41<02:35, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00009.parquet (1,745,140 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00010.parquet ...\n",
      "  Rows in file: 3,495,037\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (1,495,037 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [02:44<01:45, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00010.parquet (374,449 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00011.parquet ...\n",
      "  Rows in file: 3,351,618\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (1,351,618 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [02:47<01:12,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00011.parquet (366,330 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00012.parquet ...\n",
      "  Rows in file: 3,346,634\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (1,346,634 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [02:50<00:50,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00012.parquet (363,176 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00013.parquet ...\n",
      "  Rows in file: 3,258,822\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (1,258,822 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [02:53<00:35,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00013.parquet (359,363 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00014.parquet ...\n",
      "  Rows in file: 3,300,477\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (1,300,477 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [02:56<00:25,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00014.parquet (348,012 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00015.parquet ...\n",
      "  Rows in file: 3,256,439\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (1,256,439 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [02:59<00:17,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00015.parquet (345,232 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00016.parquet ...\n",
      "  Rows in file: 3,349,365\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (1,349,365 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [03:02<00:11,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00016.parquet (338,553 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00017.parquet ...\n",
      "  Rows in file: 3,245,473\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (1,245,473 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [03:05<00:07,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00017.parquet (340,530 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00018.parquet ...\n",
      "  Rows in file: 3,089,033\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (1,089,033 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [03:08<00:03,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00018.parquet (335,706 rows)\n",
      "\n",
      "ðŸ“‚ Processing noteRatings-00019.parquet ...\n",
      "  Rows in file: 3,102,687\n",
      "  ðŸ§© Batch 1 (2,000,000 rows)\n",
      "  ðŸ§© Batch 2 (1,102,687 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noteRatings files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:10<00:00,  9.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/note_ratings_agg_00019.parquet (342,156 rows)\n",
      "\n",
      "ðŸŽ‰ All noteRatings chunks processed safely and saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# =========================================================\n",
    "# CONFIGURATION\n",
    "# =========================================================\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = \"6\"  # limit concurrency\n",
    "\n",
    "INPUT_DIR   = \"/home/jovyan/Shared/2025-09-27-input\"\n",
    "OUTPUT_DIR  = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "CUTOFF_DATE = datetime(2023, 1, 1)\n",
    "BATCH_SIZE  = 2_000_000  # number of rows per DuckDB chunk\n",
    "\n",
    "# =========================================================\n",
    "# HELPER DEFINITIONS\n",
    "# =========================================================\n",
    "helpful_cols = [\n",
    "    \"helpfulOther\", \"helpfulClear\", \"helpfulGoodSources\",\n",
    "    \"helpfulAddressesClaim\", \"helpfulImportantContext\", \"helpfulUnbiasedLanguage\"\n",
    "]\n",
    "not_helpful_cols = [\n",
    "    \"notHelpfulIncorrect\", \"notHelpfulMissingKeyPoints\",\n",
    "    \"notHelpfulHardToUnderstand\", \"notHelpfulArgumentativeOrBiased\",\n",
    "    \"notHelpfulSpamHarassmentOrAbuse\", \"notHelpfulIrrelevantSources\",\n",
    "    \"notHelpfulOpinionSpeculation\", \"notHelpfulNoteNotNeeded\"\n",
    "]\n",
    "keep_cols = [\n",
    "    \"raterParticipantId\", \"createdAtMillis\", \"noteId\", \"ratedOnTweetId\",\n",
    "    \"agree\", \"disagree\", \"helpfulnessLevel\"\n",
    "] + helpful_cols + not_helpful_cols\n",
    "\n",
    "def safe_cast(df, cols, dtype=pl.Int64):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df = df.with_columns(pl.col(c).cast(dtype, strict=False))\n",
    "    return df\n",
    "\n",
    "# =========================================================\n",
    "# MAIN LOOP\n",
    "# =========================================================\n",
    "for i in tqdm(range(20), desc=\"Processing noteRatings files\", dynamic_ncols=True):\n",
    "    file_name = f\"noteRatings-{i:05d}.parquet\"\n",
    "    input_path = os.path.join(INPUT_DIR, file_name)\n",
    "\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"âš ï¸  Skipping missing: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nðŸ“‚ Processing {file_name} ...\")\n",
    "    con = duckdb.connect()\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM '{input_path}'\").fetchone()[0]\n",
    "    print(f\"  Rows in file: {total_rows:,}\")\n",
    "\n",
    "    chunk_results = []\n",
    "    offset = 0\n",
    "    batch_id = 0\n",
    "\n",
    "    while offset < total_rows:\n",
    "        batch_id += 1\n",
    "        query = f\"\"\"\n",
    "            SELECT {', '.join(keep_cols)}\n",
    "            FROM '{input_path}'\n",
    "            LIMIT {BATCH_SIZE} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        df = con.execute(query).pl()\n",
    "        offset += BATCH_SIZE\n",
    "        if df.is_empty():\n",
    "            continue\n",
    "\n",
    "        print(f\"  ðŸ§© Batch {batch_id} ({len(df):,} rows)\")\n",
    "\n",
    "        # --- Clean and transform ---\n",
    "        df = df.rename({\"raterParticipantId\": \"user_id\", \"createdAtMillis\": \"created_at_ms\"})\n",
    "        df = safe_cast(df, [\"created_at_ms\", \"agree\", \"disagree\"] + helpful_cols + not_helpful_cols, pl.Int64)\n",
    "\n",
    "        df = df.with_columns(pl.from_epoch(pl.col(\"created_at_ms\"), time_unit=\"ms\").alias(\"created_at\"))\n",
    "        df = df.filter(pl.col(\"created_at\") >= pl.lit(CUTOFF_DATE))\n",
    "\n",
    "        if df.is_empty():\n",
    "            continue\n",
    "\n",
    "        df = df.with_columns(pl.col(\"created_at\").dt.truncate(\"14d\").alias(\"period_start\"))\n",
    "\n",
    "        df = df.with_columns([\n",
    "            (pl.col(\"helpfulnessLevel\") == \"HELPFUL\").cast(pl.Int8).alias(\"is_helpful\"),\n",
    "            (pl.col(\"helpfulnessLevel\") == \"SOMEWHAT_HELPFUL\").cast(pl.Int8).alias(\"is_somewhat_helpful\"),\n",
    "            (pl.col(\"helpfulnessLevel\") == \"NOT_HELPFUL\").cast(pl.Int8).alias(\"is_not_helpful\")\n",
    "        ])\n",
    "\n",
    "        df = df.with_columns([\n",
    "            pl.sum_horizontal([pl.col(c) for c in helpful_cols]).alias(\"helpful_flags_sum\"),\n",
    "            pl.sum_horizontal([pl.col(c) for c in not_helpful_cols]).alias(\"not_helpful_flags_sum\")\n",
    "        ])\n",
    "\n",
    "        # --- Group and aggregate ---\n",
    "        agg = (\n",
    "            df.lazy()\n",
    "            .group_by([\"user_id\", \"period_start\"])\n",
    "            .agg([\n",
    "                pl.len().alias(\"ratings_given\"),\n",
    "                pl.n_unique(\"noteId\").alias(\"notes_rated_unique\"),\n",
    "                pl.n_unique(\"ratedOnTweetId\").alias(\"tweets_rated_unique\"),\n",
    "                pl.mean(\"agree\").alias(\"agree_rate\"),\n",
    "                pl.mean(\"disagree\").alias(\"disagree_rate\"),\n",
    "                pl.mean(\"is_helpful\").alias(\"helpful_rate\"),\n",
    "                pl.mean(\"is_somewhat_helpful\").alias(\"somewhat_helpful_rate\"),\n",
    "                pl.mean(\"is_not_helpful\").alias(\"not_helpful_rate\"),\n",
    "                pl.mean(\"helpful_flags_sum\").alias(\"avg_helpful_flags\"),\n",
    "                pl.mean(\"not_helpful_flags_sum\").alias(\"avg_not_helpful_flags\")\n",
    "            ])\n",
    "            .sort([\"user_id\", \"period_start\"])\n",
    "            .collect(streaming=True)\n",
    "        )\n",
    "\n",
    "        chunk_results.append(agg)\n",
    "\n",
    "        del df, agg\n",
    "        gc.collect()\n",
    "\n",
    "    con.close()\n",
    "\n",
    "    if len(chunk_results) == 0:\n",
    "        print(f\"âš ï¸  No valid rows in {file_name}\")\n",
    "        continue\n",
    "\n",
    "    result = pl.concat(chunk_results, how=\"vertical_relaxed\")\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"note_ratings_agg_{i:05d}.parquet\")\n",
    "    result.write_parquet(out_path)\n",
    "\n",
    "    print(f\"âœ… Saved {out_path} ({result.height:,} rows)\")\n",
    "    del result, chunk_results\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\nðŸŽ‰ All noteRatings chunks processed safely and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b982de-9feb-4538-8a5c-4e3ac857effa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Found 20 aggregated rating files to merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00000.parquet ...\n",
      "  Rows in file: 1,720,180\n",
      "  ðŸ§© Added batch 1 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:   5%|â–Œ         | 1/20 [00:01<00:21,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 2 (720,180 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00001.parquet ...\n",
      "  Rows in file: 1,724,554\n",
      "  ðŸ§© Added batch 1 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  10%|â–ˆ         | 2/20 [00:02<00:18,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 2 (724,554 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00002.parquet ...\n",
      "  Rows in file: 1,728,678\n",
      "  ðŸ§© Added batch 1 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  15%|â–ˆâ–Œ        | 3/20 [00:03<00:17,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 2 (728,678 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00003.parquet ...\n",
      "  Rows in file: 1,730,174\n",
      "  ðŸ§© Added batch 1 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  20%|â–ˆâ–ˆ        | 4/20 [00:04<00:15,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 2 (730,174 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00004.parquet ...\n",
      "  Rows in file: 1,741,006\n",
      "  ðŸ§© Added batch 1 (1,000,000 rows)\n",
      "  ðŸ§© Added batch 2 (741,006 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:09<00:37,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ’¾ Flushed 8,644,592 rows â†’ /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/tmp_merge/merged_part_000.parquet\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00005.parquet ...\n",
      "  Rows in file: 1,744,671\n",
      "  ðŸ§© Added batch 1 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:10<00:28,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 2 (744,671 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00006.parquet ...\n",
      "  Rows in file: 1,744,438\n",
      "  ðŸ§© Added batch 1 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:11<00:21,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 2 (744,438 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00007.parquet ...\n",
      "  Rows in file: 1,740,474\n",
      "  ðŸ§© Added batch 1 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:12<00:17,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 2 (740,474 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00008.parquet ...\n",
      "  Rows in file: 1,736,683\n",
      "  ðŸ§© Added batch 1 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:14,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 2 (736,683 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00009.parquet ...\n",
      "  Rows in file: 1,745,140\n",
      "  ðŸ§© Added batch 1 (1,000,000 rows)\n",
      "  ðŸ§© Added batch 2 (745,140 rows)\n",
      "  ðŸ’¾ Flushed 8,711,406 rows â†’ /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/tmp_merge/merged_part_001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:17<00:23,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00010.parquet ...\n",
      "  Rows in file: 374,449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:18<00:15,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 1 (374,449 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00011.parquet ...\n",
      "  Rows in file: 366,330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:18<00:10,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 1 (366,330 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00012.parquet ...\n",
      "  Rows in file: 363,176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:19<00:07,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 1 (363,176 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00013.parquet ...\n",
      "  Rows in file: 359,363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:19<00:04,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 1 (359,363 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00014.parquet ...\n",
      "  Rows in file: 348,012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:19<00:03,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 1 (348,012 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00015.parquet ...\n",
      "  Rows in file: 345,232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:20<00:02,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 1 (345,232 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00016.parquet ...\n",
      "  Rows in file: 338,553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:20<00:01,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 1 (338,553 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00017.parquet ...\n",
      "  Rows in file: 340,530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:20<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 1 (340,530 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00018.parquet ...\n",
      "  Rows in file: 335,706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:21<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§© Added batch 1 (335,706 rows)\n",
      "\n",
      "ðŸ“‚ Reading note_ratings_agg_00019.parquet ...\n",
      "  Rows in file: 342,156\n",
      "  ðŸ§© Added batch 1 (342,156 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging aggregated parquets: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:23<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ’¾ Flushed 3,513,507 rows â†’ /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/tmp_merge/merged_part_002.parquet\n",
      "\n",
      "ðŸ§¾ Wrote 3 intermediate parquet chunks to /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/tmp_merge\n",
      "\n",
      "ðŸš€ Consolidating all intermediate parts into final parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d612c540a5f64253b6b374dfdc5a87ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ‰ Successfully merged all aggregated files into:\n",
      "/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/note_ratings_2023_aggregated.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================================================\n",
    "# CONFIGURATION\n",
    "# =========================================================\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = \"6\"\n",
    "\n",
    "RATINGS_DIR = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings\"\n",
    "TEMP_DIR    = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/ratings/tmp_merge\"\n",
    "FINAL_PATH  = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/note_ratings_2023_aggregated.parquet\"\n",
    "\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "BATCH_SIZE  = 1_000_000  # adjustable\n",
    "gc.collect()\n",
    "\n",
    "# =========================================================\n",
    "# FIND INPUT FILES\n",
    "# =========================================================\n",
    "files = sorted([\n",
    "    os.path.join(RATINGS_DIR, f)\n",
    "    for f in os.listdir(RATINGS_DIR)\n",
    "    if f.endswith(\".parquet\") and f.startswith(\"note_ratings_agg_\")\n",
    "])\n",
    "\n",
    "print(f\"ðŸ“¦ Found {len(files)} aggregated rating files to merge\")\n",
    "\n",
    "# =========================================================\n",
    "# STAGE 1: BATCHED INTERMEDIATE PARQUETS\n",
    "# =========================================================\n",
    "tmp_counter = 0\n",
    "merged_batches = []\n",
    "\n",
    "for fpath in tqdm(files, desc=\"Merging aggregated parquets\", dynamic_ncols=True):\n",
    "    print(f\"\\nðŸ“‚ Reading {os.path.basename(fpath)} ...\")\n",
    "\n",
    "    con = duckdb.connect()\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM '{fpath}'\").fetchone()[0]\n",
    "    print(f\"  Rows in file: {total_rows:,}\")\n",
    "\n",
    "    offset = 0\n",
    "    batch_id = 0\n",
    "\n",
    "    while offset < total_rows:\n",
    "        batch_id += 1\n",
    "        query = f\"SELECT * FROM '{fpath}' LIMIT {BATCH_SIZE} OFFSET {offset}\"\n",
    "        df = con.execute(query).pl()\n",
    "        offset += BATCH_SIZE\n",
    "\n",
    "        if df.is_empty():\n",
    "            continue\n",
    "\n",
    "        merged_batches.append(df)\n",
    "        print(f\"  ðŸ§© Added batch {batch_id} ({len(df):,} rows)\")\n",
    "\n",
    "        # Flush every ~10 batches to temporary parquet\n",
    "        if len(merged_batches) >= 10:\n",
    "            combined = pl.concat(merged_batches, how=\"vertical_relaxed\")\n",
    "            tmp_path = os.path.join(TEMP_DIR, f\"merged_part_{tmp_counter:03d}.parquet\")\n",
    "            combined.write_parquet(tmp_path)\n",
    "            print(f\"  ðŸ’¾ Flushed {combined.height:,} rows â†’ {tmp_path}\")\n",
    "            tmp_counter += 1\n",
    "            del merged_batches[:]\n",
    "            del combined\n",
    "            gc.collect()\n",
    "\n",
    "    con.close()\n",
    "    gc.collect()\n",
    "\n",
    "# Final flush for remaining batches\n",
    "if merged_batches:\n",
    "    combined = pl.concat(merged_batches, how=\"vertical_relaxed\")\n",
    "    tmp_path = os.path.join(TEMP_DIR, f\"merged_part_{tmp_counter:03d}.parquet\")\n",
    "    combined.write_parquet(tmp_path)\n",
    "    print(f\"âœ… Final flush ({combined.height:,} rows) â†’ {tmp_path}\")\n",
    "    tmp_counter += 1\n",
    "    del merged_batches[:]\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nðŸ§¾ Wrote {tmp_counter} intermediate parquet chunks to {TEMP_DIR}\")\n",
    "\n",
    "# =========================================================\n",
    "# STAGE 2: FINAL CONSOLIDATION WITH DUCKDB\n",
    "# =========================================================\n",
    "print(\"\\nðŸš€ Consolidating all intermediate parts into final parquet...\")\n",
    "\n",
    "tmp_files = sorted([\n",
    "    os.path.join(TEMP_DIR, f) for f in os.listdir(TEMP_DIR) if f.endswith(\".parquet\")\n",
    "])\n",
    "\n",
    "con = duckdb.connect()\n",
    "query = f\"\"\"\n",
    "COPY (\n",
    "    SELECT * FROM read_parquet({tmp_files})\n",
    ") TO '{FINAL_PATH}' (FORMAT PARQUET);\n",
    "\"\"\"\n",
    "con.execute(query)\n",
    "con.close()\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Successfully merged all aggregated files into:\\n{FINAL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9293e5-e1f2-4d1c-88d3-ccf0a23f6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = \"6\"\n",
    "\n",
    "INPUT_PATH  = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/note_ratings_2023_aggregated.parquet\"\n",
    "TEMP_DIR    = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/tmp_reagg\"\n",
    "OUTPUT_PATH = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/note_ratings_2023_reaggregated.parquet\"\n",
    "\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "BATCH_SIZE  = 2_000_000  # adjust if needed\n",
    "gc.collect()\n",
    "\n",
    "# =========================================================\n",
    "# STAGE 1 â€” PARTIAL REAGGREGATION PER BATCH\n",
    "# =========================================================\n",
    "con = duckdb.connect()\n",
    "total_rows = con.execute(f\"SELECT COUNT(*) FROM '{INPUT_PATH}'\").fetchone()[0]\n",
    "print(f\"ðŸ“¦ Total rows in file: {total_rows:,}\")\n",
    "\n",
    "offset = 0\n",
    "batch_id = 0\n",
    "tmp_paths = []\n",
    "\n",
    "while offset < total_rows:\n",
    "    batch_id += 1\n",
    "    query = f\"SELECT * FROM '{INPUT_PATH}' LIMIT {BATCH_SIZE} OFFSET {offset}\"\n",
    "    df = con.execute(query).pl()\n",
    "    offset += BATCH_SIZE\n",
    "\n",
    "    if df.is_empty():\n",
    "        continue\n",
    "\n",
    "    print(f\"ðŸ§© Processing batch {batch_id} ({len(df):,} rows)\")\n",
    "\n",
    "    # --- Local groupby to shrink batch size ---\n",
    "    agg = (\n",
    "        df.lazy()\n",
    "        .group_by([\"user_id\", \"period_start\"])\n",
    "        .agg([\n",
    "            pl.sum(\"ratings_given\").alias(\"ratings_given\"),\n",
    "            pl.sum(\"notes_rated_unique\").alias(\"notes_rated_unique\"),\n",
    "            pl.sum(\"tweets_rated_unique\").alias(\"tweets_rated_unique\"),\n",
    "            pl.mean(\"agree_rate\").alias(\"agree_rate\"),\n",
    "            pl.mean(\"disagree_rate\").alias(\"disagree_rate\"),\n",
    "            pl.mean(\"helpful_rate\").alias(\"helpful_rate\"),\n",
    "            pl.mean(\"somewhat_helpful_rate\").alias(\"somewhat_helpful_rate\"),\n",
    "            pl.mean(\"not_helpful_rate\").alias(\"not_helpful_rate\"),\n",
    "            pl.mean(\"avg_helpful_flags\").alias(\"avg_helpful_flags\"),\n",
    "            pl.mean(\"avg_not_helpful_flags\").alias(\"avg_not_helpful_flags\")\n",
    "        ])\n",
    "        .sort([\"user_id\", \"period_start\"])\n",
    "        .collect(streaming=True)\n",
    "    )\n",
    "\n",
    "    tmp_path = os.path.join(TEMP_DIR, f\"reagg_part_{batch_id:03d}.parquet\")\n",
    "    agg.write_parquet(tmp_path)\n",
    "    tmp_paths.append(tmp_path)\n",
    "\n",
    "    print(f\"  ðŸ’¾ Wrote partial reagg â†’ {tmp_path} ({agg.height:,} rows)\")\n",
    "\n",
    "    del df, agg\n",
    "    gc.collect()\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(f\"\\nðŸ§¾ Wrote {len(tmp_paths)} temporary partial reaggregations to {TEMP_DIR}\")\n",
    "\n",
    "# =========================================================\n",
    "# STAGE 2 â€” FINAL MERGE & GLOBAL REAGGREGATION\n",
    "# =========================================================\n",
    "print(\"\\nðŸš€ Merging partial reaggregations into final dataset ...\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "query = f\"\"\"\n",
    "COPY (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        period_start,\n",
    "        SUM(ratings_given) AS ratings_given,\n",
    "        SUM(notes_rated_unique) AS notes_rated_unique,\n",
    "        SUM(tweets_rated_unique) AS tweets_rated_unique,\n",
    "        AVG(agree_rate) AS agree_rate,\n",
    "        AVG(disagree_rate) AS disagree_rate,\n",
    "        AVG(helpful_rate) AS helpful_rate,\n",
    "        AVG(somewhat_helpful_rate) AS somewhat_helpful_rate,\n",
    "        AVG(not_helpful_rate) AS not_helpful_rate,\n",
    "        AVG(avg_helpful_flags) AS avg_helpful_flags,\n",
    "        AVG(avg_not_helpful_flags) AS avg_not_helpful_flags\n",
    "    FROM read_parquet({tmp_paths})\n",
    "    GROUP BY user_id, period_start\n",
    ") TO '{OUTPUT_PATH}' (FORMAT PARQUET);\n",
    "\"\"\"\n",
    "con.execute(query)\n",
    "con.close()\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Global reaggregation complete!\\nFinal file saved to:\\n{OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d8f560-0b9e-4f9c-aaa8-5debf52855d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Found 11 partial aggregates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 partial merges:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672c84108a7049abb9f6db44c210d2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 partial merges:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:34<01:09, 34.55s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb12f196ec249b8b1a0012fa296745e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 partial merges: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:02<00:00, 20.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Stage-2 wrote 3 mid-level files\n",
      "ðŸš€ Running final external aggregation ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c05aa9b89f417497e0c83020481502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Final reaggregation complete â†’ /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/note_ratings_2023_reaggregated.parquet\n"
     ]
    }
   ],
   "source": [
    "import os, gc, duckdb, polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = \"6\"\n",
    "\n",
    "TEMP_DIR    = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/tmp_reagg\"\n",
    "OUTPUT_PATH = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/note_ratings_2023_reaggregated.parquet\"\n",
    "\n",
    "# All partial parquet paths\n",
    "tmp_files = sorted([\n",
    "    os.path.join(TEMP_DIR, f) for f in os.listdir(TEMP_DIR)\n",
    "    if f.endswith(\".parquet\")\n",
    "])\n",
    "\n",
    "print(f\"ðŸ§¾ Found {len(tmp_files)} partial aggregates\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1ï¸âƒ£  Incremental merge in small groups to limit memory\n",
    "# ---------------------------------------------------------\n",
    "INTERMEDIATE = os.path.join(TEMP_DIR, \"stage2_parts\")\n",
    "os.makedirs(INTERMEDIATE, exist_ok=True)\n",
    "group_size = 5               # merge 5 temp files at a time\n",
    "stage2_parts = []\n",
    "\n",
    "for i in tqdm(range(0, len(tmp_files), group_size), desc=\"Stage-2 partial merges\"):\n",
    "    subset = tmp_files[i:i+group_size]\n",
    "    out_path = os.path.join(INTERMEDIATE, f\"stage2_{i//group_size:03d}.parquet\")\n",
    "\n",
    "    con = duckdb.connect()\n",
    "    # Force external processing and temp directory for spills\n",
    "    con.execute(f\"PRAGMA memory_limit='2GB';\")\n",
    "    con.execute(f\"PRAGMA temp_directory='{TEMP_DIR}';\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    COPY (\n",
    "        SELECT\n",
    "            user_id,\n",
    "            period_start,\n",
    "            SUM(ratings_given) AS ratings_given,\n",
    "            SUM(notes_rated_unique) AS notes_rated_unique,\n",
    "            SUM(tweets_rated_unique) AS tweets_rated_unique,\n",
    "            AVG(agree_rate) AS agree_rate,\n",
    "            AVG(disagree_rate) AS disagree_rate,\n",
    "            AVG(helpful_rate) AS helpful_rate,\n",
    "            AVG(somewhat_helpful_rate) AS somewhat_helpful_rate,\n",
    "            AVG(not_helpful_rate) AS not_helpful_rate,\n",
    "            AVG(avg_helpful_flags) AS avg_helpful_flags,\n",
    "            AVG(avg_not_helpful_flags) AS avg_not_helpful_flags\n",
    "        FROM read_parquet({subset})\n",
    "        GROUP BY user_id, period_start\n",
    "    ) TO '{out_path}' (FORMAT PARQUET);\n",
    "    \"\"\"\n",
    "    con.execute(query)\n",
    "    con.close()\n",
    "\n",
    "    stage2_parts.append(out_path)\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"âœ… Stage-2 wrote {len(stage2_parts)} mid-level files\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2ï¸âƒ£  Final consolidation (tiny set now)\n",
    "# ---------------------------------------------------------\n",
    "print(\"ðŸš€ Running final external aggregation ...\")\n",
    "con = duckdb.connect()\n",
    "con.execute(f\"PRAGMA memory_limit='2GB';\")\n",
    "con.execute(f\"PRAGMA temp_directory='{TEMP_DIR}';\")\n",
    "\n",
    "query = f\"\"\"\n",
    "COPY (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        period_start,\n",
    "        SUM(ratings_given) AS ratings_given,\n",
    "        SUM(notes_rated_unique) AS notes_rated_unique,\n",
    "        SUM(tweets_rated_unique) AS tweets_rated_unique,\n",
    "        AVG(agree_rate) AS agree_rate,\n",
    "        AVG(disagree_rate) AS disagree_rate,\n",
    "        AVG(helpful_rate) AS helpful_rate,\n",
    "        AVG(somewhat_helpful_rate) AS somewhat_helpful_rate,\n",
    "        AVG(not_helpful_rate) AS not_helpful_rate,\n",
    "        AVG(avg_helpful_flags) AS avg_helpful_flags,\n",
    "        AVG(avg_not_helpful_flags) AS avg_not_helpful_flags\n",
    "    FROM read_parquet({stage2_parts})\n",
    "    GROUP BY user_id, period_start\n",
    ") TO '{OUTPUT_PATH}' (FORMAT PARQUET);\n",
    "\"\"\"\n",
    "con.execute(query)\n",
    "con.close()\n",
    "\n",
    "print(f\"ðŸŽ‰ Final reaggregation complete â†’ {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e95def-3f26-4fdc-9160-480c0d2dcc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Total rows in file: 20,869,319\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = \"6\"\n",
    "\n",
    "\n",
    "INPUT_PATH = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/aggregates/note_ratings_2023_reaggregated.parquet\"\n",
    "\n",
    "con = duckdb.connect()\n",
    "total_rows = con.execute(f\"SELECT COUNT(1) FROM '{INPUT_PATH}'\").fetchone()[0]\n",
    "print(f\"ðŸ“¦ Total rows in file: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f66d64b-1135-4ae1-9757-2316c365828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ratings rows: 20,869,319\n",
      "ðŸ§© Joining ratings batch 1 (offset 0) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfa239f291c493d97df07a6858475e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Joining ratings batch 2 (offset 2,000,000) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad80d174ce94981839048a9944fe714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Joining ratings batch 3 (offset 4,000,000) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad72430b0d84c52af92ca28eb81fd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Joining ratings batch 4 (offset 6,000,000) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f777c06adf004cbd82ff1bab3f99c4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Joining ratings batch 5 (offset 8,000,000) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cd33c30f0d45a49c9707433edc7371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Joining ratings batch 6 (offset 10,000,000) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c948ab7e4b35427283aa4ca684344656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Joining ratings batch 7 (offset 12,000,000) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0a9f79f7304c0f9f46c3cb7f3bd910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Joining ratings batch 8 (offset 14,000,000) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52141b5acfd4698aa3bbca2abb414e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Joining ratings batch 9 (offset 16,000,000) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589945765c4d4e2ca6d40706a7ba7fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Joining ratings batch 10 (offset 18,000,000) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff9e2a5d2d247e1adb1739d199d39fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Joining ratings batch 11 (offset 20,000,000) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9b7fb113514e8eb98f700ed891fa28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wrote 11 joined chunks to /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/aggregates/super_merge_tmp2\n",
      "ðŸš€ Consolidating all joined parts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed617b20790a4695b36007cdb6cf328e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Master dataset ready â†’ /home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/aggregates/user_period_master.parquet\n"
     ]
    }
   ],
   "source": [
    "import os, gc, duckdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = \"6\"\n",
    "\n",
    "BASE_DIR = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/aggregates\"\n",
    "RATINGS  = os.path.join(BASE_DIR, \"note_ratings_2023_reaggregated.parquet\")\n",
    "NOTES    = os.path.join(BASE_DIR, \"notes_writers_2023_aggregated.parquet\")\n",
    "STATUS   = os.path.join(BASE_DIR, \"note_status_history_2023_aggregated.parquet\")\n",
    "ENROLL   = os.path.join(BASE_DIR, \"user_enrollment_2023_aggregated.parquet\")\n",
    "REQUESTS = os.path.join(BASE_DIR, \"note_requests_2023_aggregated.parquet\")\n",
    "\n",
    "TEMP_DIR  = os.path.join(BASE_DIR, \"super_merge_tmp2\")\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "MEM_LIMIT_GB = 2\n",
    "BATCH_SIZE   = 2_000_000   # tune down if memory tight\n",
    "part_files   = []\n",
    "\n",
    "con = duckdb.connect()\n",
    "total_rows = con.execute(f\"SELECT COUNT(*) FROM '{RATINGS}'\").fetchone()[0]\n",
    "con.close()\n",
    "print(f\"Total ratings rows: {total_rows:,}\")\n",
    "\n",
    "offset = 0\n",
    "batch_idx = 0\n",
    "\n",
    "while offset < total_rows:\n",
    "    batch_idx += 1\n",
    "    out_path = os.path.join(TEMP_DIR, f\"merged_part_{batch_idx:03d}.parquet\")\n",
    "\n",
    "    con = duckdb.connect()\n",
    "    con.execute(f\"PRAGMA memory_limit='{MEM_LIMIT_GB}GB';\")\n",
    "    con.execute(f\"PRAGMA temp_directory='{TEMP_DIR}';\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    COPY (\n",
    "        WITH r AS (\n",
    "            SELECT * FROM read_parquet('{RATINGS}')\n",
    "            LIMIT {BATCH_SIZE} OFFSET {offset}\n",
    "        )\n",
    "        SELECT \n",
    "            r.*,\n",
    "            n.* EXCLUDE (user_id, period_start),\n",
    "            s.* EXCLUDE (user_id, period_start),\n",
    "            e.* EXCLUDE (user_id, period_start),\n",
    "            q.* EXCLUDE (user_id, period_start)\n",
    "        FROM r\n",
    "        LEFT JOIN read_parquet('{NOTES}')   n USING (user_id, period_start)\n",
    "        LEFT JOIN read_parquet('{STATUS}')  s USING (user_id, period_start)\n",
    "        LEFT JOIN read_parquet('{ENROLL}')  e USING (user_id, period_start)\n",
    "        LEFT JOIN read_parquet('{REQUESTS}') q USING (user_id, period_start)\n",
    "    ) TO '{out_path}' (FORMAT PARQUET);\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ§© Joining ratings batch {batch_idx} (offset {offset:,}) ...\")\n",
    "    con.execute(query)\n",
    "    con.close()\n",
    "\n",
    "    part_files.append(out_path)\n",
    "    offset += BATCH_SIZE\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"âœ… Wrote {len(part_files)} joined chunks to {TEMP_DIR}\")\n",
    "\n",
    "# -------- final consolidation (external mode) --------\n",
    "print(\"ðŸš€ Consolidating all joined parts...\")\n",
    "con = duckdb.connect()\n",
    "con.execute(f\"PRAGMA memory_limit='{MEM_LIMIT_GB}GB';\")\n",
    "con.execute(f\"PRAGMA temp_directory='{TEMP_DIR}';\")\n",
    "final_path = os.path.join(BASE_DIR, \"user_period_master.parquet\")\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  SELECT * FROM read_parquet({part_files})\n",
    ") TO '{final_path}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "con.close()\n",
    "\n",
    "print(f\"ðŸŽ‰ Master dataset ready â†’ {final_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc72bdde-d9e4-4869-8bad-1fadf19aba55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'period_start', 'ratings_given', 'notes_rated_unique',\n",
      "       'tweets_rated_unique', 'agree_rate', 'disagree_rate', 'helpful_rate',\n",
      "       'somewhat_helpful_rate', 'not_helpful_rate', 'avg_helpful_flags',\n",
      "       'avg_not_helpful_flags', 'notes_written', 'num_misleading',\n",
      "       'num_not_misleading', 'avg_trustworthySources', 'media_note_ratio',\n",
      "       'misleading_flag_sum', 'not_misleading_flag_sum', 'share_misleading',\n",
      "       'avg_flags_per_note', 'notes_with_status', 'notes_helpful',\n",
      "       'notes_not_helpful', 'notes_nmr', 'notes_locked', 'first_helpful',\n",
      "       'latest_helpful', 'latest_not_helpful', 'avg_days_to_first_nonNMR',\n",
      "       'avg_days_to_lock', 'share_helpful', 'share_not_helpful', 'records',\n",
      "       'avg_successfulRatingNeededToEarnIn', 'avg_days_since_state_change',\n",
      "       'avg_days_since_earnout', 'is_new_user', 'is_earned_in', 'is_at_risk',\n",
      "       'is_earned_out', 'has_ever_earned_out', 'is_core_population',\n",
      "       'avg_modelingGroup', 'notes_requested', 'unique_tweets_requested',\n",
      "       'with_source_link', 'share_with_link'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "path = \"/home/jovyan/Shared/project1-group1/info-470-project-1/langdata/time-series-core/aggregates/user_period_master.parquet\"\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Read the parquet into DuckDB\n",
    "df = con.execute(f\"SELECT * FROM read_parquet('{path}') LIMIT 0\").fetchdf()\n",
    "\n",
    "# Show columns\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860cd22f-b51a-4a79-aeae-f23c481a8f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
