{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a503c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\wongb\\anaconda3\\lib\\site-packages (3.12.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\wongb\\appdata\\roaming\\python\\python313\\site-packages (from h5py) (2.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f57f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating 3D tensor dataset (High Memory Mode - 20GB)...\n",
      "============================================================\n",
      "üìä Step 1: Loading complete dataset...\n",
      "‚ö° Using high-memory mode for maximum speed...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9802b949bd342f6bde2df333563bca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 21,020,118 records\n",
      "Tensor dimensions: 1,279,178 √ó 72 √ó 68\n",
      "Total elements: 6,262,855,488\n",
      "\n",
      "üéØ Step 2: Loading factor1 labels...\n",
      "Tensor dimensions: 1,279,178 √ó 72 √ó 68\n",
      "Total elements: 6,262,855,488\n",
      "\n",
      "üéØ Step 2: Loading factor1 labels...\n",
      "Labels assigned: 422,979 users (33.1%)\n",
      "\n",
      "üîß Step 3: Building tensor in memory...\n",
      "Labels assigned: 422,979 users (33.1%)\n",
      "\n",
      "üîß Step 3: Building tensor in memory...\n",
      "‚ö° Using vectorized processing...\n",
      "‚ö° Using vectorized processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing users:   0%|          | 0/1279178 [00:27<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'NAType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     74\u001b[39m period_idx = period_to_idx[row[\u001b[33m'\u001b[39m\u001b[33mperiod_start\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Extract all feature values at once\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m feature_values = \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m feature_values[pd.isna(feature_values)] = np.nan\n\u001b[32m     80\u001b[39m tensor[user_idx, period_idx, :] = feature_values\n",
      "\u001b[31mTypeError\u001b[39m: float() argument must be a string or a real number, not 'NAType'"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "master_path = r\"C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\aggregator\\data\\user_period_master_complete_with_authored_scores.parquet\"\n",
    "factor_data_path = r\"C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\aggregator\\data\\prescoringRaterModelOutput_3dim.tsv\"\n",
    "output_path = r\"C:\\Users\\wongb\\twitter-community-notes-time-series\\twitter-community-notes-user-time-series\\user_factor_analysis\\data\\tensordata\\user_timeseries_tensor.h5\"\n",
    "\n",
    "print(\"üöÄ Creating 3D tensor dataset (High Memory Mode - 20GB)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Connect to DuckDB with high memory\n",
    "con = duckdb.connect()\n",
    "con.execute(\"PRAGMA memory_limit='18GB';\")  # Leave 2GB for other operations\n",
    "\n",
    "# Step 1: Load entire dataset into memory at once\n",
    "print(\"üìä Step 1: Loading complete dataset...\")\n",
    "print(\"‚ö° Using high-memory mode for maximum speed...\")\n",
    "\n",
    "full_data = con.execute(f\"\"\"\n",
    "    SELECT * FROM '{master_path}' ORDER BY userId, period_start\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(full_data):,} records\")\n",
    "\n",
    "# Get dimensions\n",
    "unique_users = full_data['userId'].unique()\n",
    "unique_periods = sorted(full_data['period_start'].unique())\n",
    "feature_columns = [col for col in full_data.columns if col not in ['userId', 'period_start', 'period_end']]\n",
    "\n",
    "n_users = len(unique_users)\n",
    "n_timesteps = len(unique_periods)\n",
    "n_features = len(feature_columns)\n",
    "\n",
    "print(f\"Tensor dimensions: {n_users:,} √ó {n_timesteps} √ó {n_features}\")\n",
    "print(f\"Total elements: {n_users * n_timesteps * n_features:,}\")\n",
    "\n",
    "# Step 2: Load factor1 labels (all at once)\n",
    "print(\"\\nüéØ Step 2: Loading factor1 labels...\")\n",
    "factor_df = pd.read_csv(factor_data_path, sep='\\t')\n",
    "\n",
    "# Filter out rows where internalRaterFactor1 is null\n",
    "factor_df_clean = factor_df.dropna(subset=['internalRaterFactor1'])\n",
    "factor_dict = dict(zip(factor_df_clean['raterParticipantId'], factor_df_clean['internalRaterFactor1']))\n",
    "\n",
    "print(f\"Total factor records: {len(factor_df):,}\")\n",
    "print(f\"Records with non-null factor1: {len(factor_df_clean):,}\")\n",
    "print(f\"Null factor1 percentage: {((len(factor_df) - len(factor_df_clean)) / len(factor_df) * 100):.1f}%\")\n",
    "\n",
    "# Create labels array\n",
    "user_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}\n",
    "labels = np.full(n_users, np.nan, dtype=np.float32)\n",
    "labeled_count = 0\n",
    "\n",
    "for user_id, factor1 in factor_dict.items():\n",
    "    if user_id in user_to_idx:\n",
    "        labels[user_to_idx[user_id]] = factor1\n",
    "        labeled_count += 1\n",
    "\n",
    "print(f\"Labels assigned: {labeled_count:,} users ({labeled_count/n_users*100:.1f}%)\")\n",
    "\n",
    "# Step 3: Create tensor in memory (fastest approach)\n",
    "print(\"\\nüîß Step 3: Building tensor in memory...\")\n",
    "tensor = np.full((n_users, n_timesteps, n_features), np.nan, dtype=np.float32)\n",
    "\n",
    "# Create period mapping\n",
    "period_to_idx = {period: idx for idx, period in enumerate(unique_periods)}\n",
    "\n",
    "# Vectorized approach - group by user and period for batch processing\n",
    "print(\"‚ö° Using vectorized processing...\")\n",
    "\n",
    "# Process all data at once using pandas pivot operations\n",
    "for i, (user_id, user_data) in enumerate(tqdm(full_data.groupby('userId'), desc=\"Processing users\")):\n",
    "    user_idx = user_to_idx[user_id]\n",
    "    \n",
    "    for _, row in user_data.iterrows():\n",
    "        period_idx = period_to_idx[row['period_start']]\n",
    "        \n",
    "        # Extract feature values with proper null handling\n",
    "        feature_values = np.full(n_features, np.nan, dtype=np.float32)\n",
    "        for j, col in enumerate(feature_columns):\n",
    "            val = row[col]\n",
    "            if pd.notna(val) and val is not pd.NA:\n",
    "                try:\n",
    "                    feature_values[j] = float(val)\n",
    "                except (ValueError, TypeError):\n",
    "                    feature_values[j] = np.nan\n",
    "            else:\n",
    "                feature_values[j] = np.nan\n",
    "        \n",
    "        tensor[user_idx, period_idx, :] = feature_values\n",
    "\n",
    "# Step 4: Data quality analysis\n",
    "print(\"\\nüìà Step 4: Data quality analysis...\")\n",
    "total_elements = tensor.size\n",
    "missing_elements = np.sum(np.isnan(tensor))\n",
    "fill_rate = (total_elements - missing_elements) / total_elements\n",
    "\n",
    "print(f\"Total tensor elements: {total_elements:,}\")\n",
    "print(f\"Missing elements: {missing_elements:,}\")\n",
    "print(f\"Fill rate: {fill_rate:.1%}\")\n",
    "\n",
    "# Step 5: Save to HDF5 (all at once - fastest)\n",
    "print(\"\\nüíæ Step 5: Saving to HDF5...\")\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with h5py.File(output_path, 'w') as f:\n",
    "    # Save tensor (single write operation)\n",
    "    print(\"  üíΩ Writing tensor...\")\n",
    "    f.create_dataset('tensor', data=tensor, compression='gzip', compression_opts=6)\n",
    "    \n",
    "    # Save metadata\n",
    "    print(\"  üìã Writing metadata...\")\n",
    "    user_ids_bytes = [str(uid).encode('utf-8') for uid in unique_users]\n",
    "    f.create_dataset('user_ids', data=user_ids_bytes)\n",
    "    f.create_dataset('factor1_labels', data=labels)\n",
    "    \n",
    "    period_strings = [str(p).encode('utf-8') for p in unique_periods]\n",
    "    f.create_dataset('periods', data=period_strings)\n",
    "    \n",
    "    feature_names_bytes = [name.encode('utf-8') for name in feature_columns]\n",
    "    f.create_dataset('feature_names', data=feature_names_bytes)\n",
    "    \n",
    "    # Attributes\n",
    "    f.attrs['n_users'] = n_users\n",
    "    f.attrs['n_timesteps'] = n_timesteps\n",
    "    f.attrs['n_features'] = n_features\n",
    "    f.attrs['fill_rate'] = fill_rate\n",
    "    f.attrs['labeled_users'] = labeled_count\n",
    "    f.attrs['creation_date'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    f.attrs['memory_mode'] = 'high_memory_20gb'\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(\"\\n‚úÖ High-speed tensor creation complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ File: {output_path}\")\n",
    "print(f\"üìä Size: {os.path.getsize(output_path) / (1024**2):.1f} MB\")\n",
    "print(f\"üè∑Ô∏è Labeled users: {labeled_count:,}/{n_users:,}\")\n",
    "print(f\"‚ö° Processing mode: High-memory (20GB)\")\n",
    "\n",
    "# Verification\n",
    "with h5py.File(output_path, 'r') as f:\n",
    "    print(f\"\\nüîç Verification:\")\n",
    "    print(f\"  Tensor shape: {f['tensor'].shape}\")\n",
    "    print(f\"  Tensor dtype: {f['tensor'].dtype}\")\n",
    "    print(f\"  First user: {f['user_ids'][0].decode('utf-8')}\")\n",
    "    print(f\"  Labels: {np.sum(~np.isnan(f['factor1_labels'][:]))} non-null\")\n",
    "    print(f\"  Fill rate: {f.attrs['fill_rate']:.1%}\")\n",
    "\n",
    "print(\"\\nüéØ Ready for high-performance time series ML!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841be924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
